{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Code for using FiftyOne to train a Faster RCNN on COCO data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x17d9abed8d0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import torchvision.models.detection.roi_heads\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "from dataset import FiftyOneTorchDataset, get_transforms\n",
    "from model import create_model\n",
    "from utils import add_detections\n",
    "\n",
    "from engine import train_model\n",
    "import config\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full dataset from model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5crNDNsRWdPT",
    "outputId": "4f3ff734-ca0a-4312-a811-7f84db514fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\blain\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\blain\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [19.9s elapsed, 0s remaining, 303.1 samples/s]      \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x17dd3d2b310>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=5e4c5749-2554-4138-a693-f827f5920d03\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lodad in the dataset from the FiftyOne model Zoo\n",
    "fo_dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")\n",
    "\n",
    "#needed to calculate image height and width\n",
    "fo_dataset.compute_metadata()\n",
    "\n",
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqU6Ckq4WKHK"
   },
   "source": [
    "For example, cluttered images make it difficult for models to localize objects. We can use FiftyOne to create a view containing only samples with more than, say, 10 objects. You can perform the same operations on views as datasets, so we can create an instance of our PyTorch dataset from this view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kLACOukJFUxd"
   },
   "outputs": [],
   "source": [
    "#if we want to see images with more than 10 items, we can\n",
    "# busy_view = fo_dataset.match(F(\"ground_truth.detections\").length() > 10)\n",
    "# busy_torch_dataset = FiftyOneTorchDataset(busy_view)\n",
    "# session.view = busy_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKsE_7TOWXBE"
   },
   "source": [
    "### Create training and testing views (and corresponding PyTorch datasets) that only contain some items from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TELK0NWmWrMT",
    "outputId": "8bf582cf-e483-4643-8f6b-7c664a2d6c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning on 4000 samples\n",
      "Testing on 1000 samples\n"
     ]
    }
   ],
   "source": [
    "subset = False\n",
    "\n",
    "train_transforms, test_transforms = get_transforms()\n",
    "\n",
    "if subset:\n",
    "    # to filter certain items from the dataset we can\n",
    "    item_list = [\"car\", \"dog\", \"bus\", 'fork', 'tie', 'person']\n",
    "    item_list = ['airplane']\n",
    "    item_view = fo_dataset.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(item_list))\n",
    "\n",
    "    #session.view = item_view\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    train_view = item_view.take((len(item_view) * config.TRAIN_TEST_SPLIT), seed=51)\n",
    "    test_view = item_view.exclude([s.id for s in train_view])\n",
    "\n",
    "else:\n",
    "    train_view = fo_dataset.take(len(fo_dataset) * config.TRAIN_TEST_SPLIT)\n",
    "    test_view = fo_dataset.exclude([s.id for s in train_view])\n",
    "    item_list = fo_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "print(f'Traning on {len(train_view)} samples')\n",
    "print(f'Testing on {len(test_view)} samples')\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "train_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
    "        classes=item_list)\n",
    "evaluation_dataset = FiftyOneTorchDataset(test_view, test_transforms,\n",
    "        classes=item_list)\n",
    "\n",
    "#this is needed for later use, but not for creating the dataset\n",
    "if item_list[0] != 'background':\n",
    "     item_list.insert(0,'background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # map labels to single vehicle class\n",
    "# vehicle_list = ['car', 'bus', 'truck']\n",
    "# vehicles_map = {c: \"vehicle\" for c in vehicle_list}\n",
    "#\n",
    "# train_map_view = train_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "# test_map_view = test_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "#\n",
    "# # use our dataset and defined transformations\n",
    "# torch_map_dataset = FiftyOneTorchDataset(train_map_view, train_transforms)\n",
    "# torch_map_dataset_test = FiftyOneTorchDataset(test_map_view, test_transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5je6lVBWz5r"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# #to change the loss function, create a new function and implement like below\n",
    "# import torchvision\n",
    "# torchvision.models.detection.roi_heads.fastrcnn_loss = cliprcnn_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blain\\anaconda3\\envs\\torch-frcnn\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: [0]  [   0/1000]  eta: 0:22:47  lr: 0.000010  loss: 0.8858 (0.8858)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0617 (0.0617)  loss_objectness: 0.7377 (0.7377)  loss_rpn_box_reg: 0.0864 (0.0864)  time: 1.3673  data: 0.2691  max mem: 4717\n",
      "Training Epoch: [0]  [  10/1000]  eta: 0:08:16  lr: 0.000060  loss: 0.8588 (0.7875)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1218 (0.1250)  loss_objectness: 0.5458 (0.5511)  loss_rpn_box_reg: 0.0864 (0.1114)  time: 0.5015  data: 0.1417  max mem: 5190\n",
      "Training Epoch: [0]  [  20/1000]  eta: 0:07:14  lr: 0.000110  loss: 0.6416 (0.6753)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1185 (0.1212)  loss_objectness: 0.3375 (0.4085)  loss_rpn_box_reg: 0.1021 (0.1456)  time: 0.3968  data: 0.1279  max mem: 5190\n",
      "Training Epoch: [0]  [  30/1000]  eta: 0:06:52  lr: 0.000160  loss: 0.6087 (0.6955)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1305 (0.1449)  loss_objectness: 0.2259 (0.3619)  loss_rpn_box_reg: 0.1898 (0.1887)  time: 0.3840  data: 0.1242  max mem: 5190\n",
      "Training Epoch: [0]  [  40/1000]  eta: 0:06:41  lr: 0.000210  loss: 0.7071 (0.6786)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1391 (0.1421)  loss_objectness: 0.2172 (0.3205)  loss_rpn_box_reg: 0.2307 (0.2160)  time: 0.3919  data: 0.1222  max mem: 5190\n",
      "Training Epoch: [0]  [  50/1000]  eta: 0:06:32  lr: 0.000260  loss: 0.5279 (0.6522)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1255 (0.1409)  loss_objectness: 0.1726 (0.2915)  loss_rpn_box_reg: 0.1955 (0.2197)  time: 0.3925  data: 0.1219  max mem: 5190\n",
      "Training Epoch: [0]  [  60/1000]  eta: 0:06:25  lr: 0.000310  loss: 0.4302 (0.6291)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1253 (0.1437)  loss_objectness: 0.1535 (0.2682)  loss_rpn_box_reg: 0.1557 (0.2172)  time: 0.3934  data: 0.1210  max mem: 5190\n",
      "Training Epoch: [0]  [  70/1000]  eta: 0:06:19  lr: 0.000360  loss: 0.4124 (0.6028)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1253 (0.1445)  loss_objectness: 0.1381 (0.2504)  loss_rpn_box_reg: 0.1451 (0.2079)  time: 0.3965  data: 0.1221  max mem: 5197\n",
      "Training Epoch: [0]  [  80/1000]  eta: 0:06:14  lr: 0.000410  loss: 0.4124 (0.5795)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1113 (0.1384)  loss_objectness: 0.1338 (0.2379)  loss_rpn_box_reg: 0.1451 (0.2033)  time: 0.3963  data: 0.1222  max mem: 5197\n",
      "Training Epoch: [0]  [  90/1000]  eta: 0:06:08  lr: 0.000460  loss: 0.4045 (0.5625)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0915 (0.1360)  loss_objectness: 0.1475 (0.2314)  loss_rpn_box_reg: 0.1263 (0.1952)  time: 0.3960  data: 0.1218  max mem: 5197\n",
      "Training Epoch: [0]  [ 100/1000]  eta: 0:06:04  lr: 0.000510  loss: 0.4045 (0.5503)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1187 (0.1363)  loss_objectness: 0.1473 (0.2212)  loss_rpn_box_reg: 0.0966 (0.1928)  time: 0.4002  data: 0.1198  max mem: 5197\n",
      "Training Epoch: [0]  [ 110/1000]  eta: 0:05:58  lr: 0.000560  loss: 0.4053 (0.5402)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1358 (0.1387)  loss_objectness: 0.1306 (0.2127)  loss_rpn_box_reg: 0.1430 (0.1889)  time: 0.3885  data: 0.1178  max mem: 5197\n",
      "Training Epoch: [0]  [ 120/1000]  eta: 0:05:53  lr: 0.000610  loss: 0.4426 (0.5316)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1262 (0.1371)  loss_objectness: 0.1350 (0.2079)  loss_rpn_box_reg: 0.1452 (0.1866)  time: 0.3830  data: 0.1230  max mem: 5197\n",
      "Training Epoch: [0]  [ 130/1000]  eta: 0:05:48  lr: 0.000660  loss: 0.4426 (0.5218)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1262 (0.1380)  loss_objectness: 0.1404 (0.2028)  loss_rpn_box_reg: 0.1332 (0.1810)  time: 0.3911  data: 0.1266  max mem: 5197\n",
      "Training Epoch: [0]  [ 140/1000]  eta: 0:05:41  lr: 0.000710  loss: 0.3835 (0.5144)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1426 (0.1406)  loss_objectness: 0.1285 (0.1977)  loss_rpn_box_reg: 0.1028 (0.1761)  time: 0.3731  data: 0.1258  max mem: 5197\n",
      "Training Epoch: [0]  [ 150/1000]  eta: 0:05:37  lr: 0.000760  loss: 0.3857 (0.5102)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1601 (0.1431)  loss_objectness: 0.1299 (0.1952)  loss_rpn_box_reg: 0.0759 (0.1719)  time: 0.3711  data: 0.1253  max mem: 5197\n",
      "Training Epoch: [0]  [ 160/1000]  eta: 0:05:33  lr: 0.000810  loss: 0.3527 (0.4975)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1336 (0.1410)  loss_objectness: 0.1195 (0.1898)  loss_rpn_box_reg: 0.0775 (0.1667)  time: 0.3956  data: 0.1213  max mem: 5197\n",
      "Training Epoch: [0]  [ 170/1000]  eta: 0:05:29  lr: 0.000860  loss: 0.3449 (0.4914)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0996 (0.1402)  loss_objectness: 0.1131 (0.1869)  loss_rpn_box_reg: 0.0899 (0.1642)  time: 0.4023  data: 0.1235  max mem: 5197\n",
      "Training Epoch: [0]  [ 180/1000]  eta: 0:05:25  lr: 0.000910  loss: 0.3449 (0.4864)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1173 (0.1419)  loss_objectness: 0.1180 (0.1833)  loss_rpn_box_reg: 0.0899 (0.1612)  time: 0.3922  data: 0.1309  max mem: 5197\n",
      "Training Epoch: [0]  [ 190/1000]  eta: 0:05:21  lr: 0.000960  loss: 0.4207 (0.4831)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1631 (0.1431)  loss_objectness: 0.0975 (0.1799)  loss_rpn_box_reg: 0.1156 (0.1600)  time: 0.3901  data: 0.1303  max mem: 5197\n",
      "Training Epoch: [0]  [ 200/1000]  eta: 0:05:17  lr: 0.001010  loss: 0.4207 (0.4797)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1566 (0.1437)  loss_objectness: 0.0975 (0.1779)  loss_rpn_box_reg: 0.1156 (0.1581)  time: 0.3935  data: 0.1237  max mem: 5197\n",
      "Training Epoch: [0]  [ 210/1000]  eta: 0:05:13  lr: 0.001060  loss: 0.4110 (0.4764)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1430 (0.1427)  loss_objectness: 0.1228 (0.1765)  loss_rpn_box_reg: 0.1027 (0.1572)  time: 0.3991  data: 0.1209  max mem: 5197\n",
      "Training Epoch: [0]  [ 220/1000]  eta: 0:05:08  lr: 0.001110  loss: 0.4546 (0.4749)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1685 (0.1464)  loss_objectness: 0.1123 (0.1737)  loss_rpn_box_reg: 0.1027 (0.1547)  time: 0.3823  data: 0.1194  max mem: 5197\n",
      "Training Epoch: [0]  [ 230/1000]  eta: 0:05:03  lr: 0.001160  loss: 0.4187 (0.4729)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1574 (0.1462)  loss_objectness: 0.1149 (0.1729)  loss_rpn_box_reg: 0.1200 (0.1538)  time: 0.3666  data: 0.1181  max mem: 5197\n",
      "Training Epoch: [0]  [ 240/1000]  eta: 0:04:58  lr: 0.001210  loss: 0.4551 (0.4731)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1493 (0.1468)  loss_objectness: 0.1439 (0.1727)  loss_rpn_box_reg: 0.1258 (0.1536)  time: 0.3737  data: 0.1229  max mem: 5197\n",
      "Training Epoch: [0]  [ 250/1000]  eta: 0:04:54  lr: 0.001260  loss: 0.4439 (0.4705)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1564 (0.1462)  loss_objectness: 0.1484 (0.1723)  loss_rpn_box_reg: 0.1062 (0.1519)  time: 0.3727  data: 0.1239  max mem: 5197\n",
      "Training Epoch: [0]  [ 260/1000]  eta: 0:04:49  lr: 0.001310  loss: 0.3856 (0.4685)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1531 (0.1468)  loss_objectness: 0.1435 (0.1707)  loss_rpn_box_reg: 0.0743 (0.1509)  time: 0.3629  data: 0.1206  max mem: 5197\n",
      "Training Epoch: [0]  [ 270/1000]  eta: 0:04:44  lr: 0.001360  loss: 0.3900 (0.4676)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1712 (0.1495)  loss_objectness: 0.0933 (0.1679)  loss_rpn_box_reg: 0.0709 (0.1503)  time: 0.3556  data: 0.1205  max mem: 5197\n",
      "Training Epoch: [0]  [ 280/1000]  eta: 0:04:40  lr: 0.001410  loss: 0.4197 (0.4678)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1874 (0.1517)  loss_objectness: 0.0961 (0.1664)  loss_rpn_box_reg: 0.1042 (0.1497)  time: 0.3613  data: 0.1221  max mem: 5197\n",
      "Training Epoch: [0]  [ 290/1000]  eta: 0:04:35  lr: 0.001460  loss: 0.4810 (0.4700)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1972 (0.1534)  loss_objectness: 0.1412 (0.1667)  loss_rpn_box_reg: 0.1097 (0.1500)  time: 0.3580  data: 0.1234  max mem: 5197\n",
      "Training Epoch: [0]  [ 300/1000]  eta: 0:04:31  lr: 0.001510  loss: 0.4904 (0.4699)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1761 (0.1540)  loss_objectness: 0.1450 (0.1663)  loss_rpn_box_reg: 0.1493 (0.1496)  time: 0.3679  data: 0.1250  max mem: 5197\n",
      "Training Epoch: [0]  [ 310/1000]  eta: 0:04:26  lr: 0.001560  loss: 0.4124 (0.4697)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1606 (0.1554)  loss_objectness: 0.1416 (0.1660)  loss_rpn_box_reg: 0.1082 (0.1483)  time: 0.3749  data: 0.1234  max mem: 5197\n",
      "Training Epoch: [0]  [ 320/1000]  eta: 0:04:22  lr: 0.001610  loss: 0.4006 (0.4683)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1606 (0.1561)  loss_objectness: 0.1382 (0.1644)  loss_rpn_box_reg: 0.1082 (0.1477)  time: 0.3657  data: 0.1210  max mem: 5197\n",
      "Training Epoch: [0]  [ 330/1000]  eta: 0:04:18  lr: 0.001660  loss: 0.4365 (0.4669)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1641 (0.1566)  loss_objectness: 0.1000 (0.1632)  loss_rpn_box_reg: 0.1309 (0.1471)  time: 0.3669  data: 0.1186  max mem: 5197\n",
      "Training Epoch: [0]  [ 340/1000]  eta: 0:04:14  lr: 0.001710  loss: 0.4189 (0.4651)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1587 (0.1564)  loss_objectness: 0.1190 (0.1621)  loss_rpn_box_reg: 0.1007 (0.1466)  time: 0.3669  data: 0.1155  max mem: 5197\n",
      "Training Epoch: [0]  [ 350/1000]  eta: 0:04:10  lr: 0.001760  loss: 0.4122 (0.4655)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1448 (0.1572)  loss_objectness: 0.1275 (0.1614)  loss_rpn_box_reg: 0.0855 (0.1469)  time: 0.3720  data: 0.1189  max mem: 5197\n",
      "Training Epoch: [0]  [ 360/1000]  eta: 0:04:06  lr: 0.001810  loss: 0.4122 (0.4648)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1410 (0.1573)  loss_objectness: 0.1194 (0.1605)  loss_rpn_box_reg: 0.0982 (0.1470)  time: 0.3860  data: 0.1240  max mem: 5197\n",
      "Training Epoch: [0]  [ 370/1000]  eta: 0:04:01  lr: 0.001860  loss: 0.4274 (0.4642)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1748 (0.1586)  loss_objectness: 0.1096 (0.1593)  loss_rpn_box_reg: 0.1087 (0.1462)  time: 0.3694  data: 0.1215  max mem: 5197\n",
      "Training Epoch: [0]  [ 380/1000]  eta: 0:03:58  lr: 0.001910  loss: 0.4100 (0.4624)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1829 (0.1581)  loss_objectness: 0.1096 (0.1588)  loss_rpn_box_reg: 0.1016 (0.1455)  time: 0.3702  data: 0.1201  max mem: 5197\n",
      "Training Epoch: [0]  [ 390/1000]  eta: 0:03:54  lr: 0.001960  loss: 0.3702 (0.4614)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1426 (0.1577)  loss_objectness: 0.1251 (0.1581)  loss_rpn_box_reg: 0.1123 (0.1455)  time: 0.3814  data: 0.1219  max mem: 5197\n",
      "Training Epoch: [0]  [ 400/1000]  eta: 0:03:50  lr: 0.002010  loss: 0.3896 (0.4601)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1591 (0.1580)  loss_objectness: 0.1070 (0.1572)  loss_rpn_box_reg: 0.1146 (0.1450)  time: 0.3721  data: 0.1220  max mem: 5197\n",
      "Training Epoch: [0]  [ 410/1000]  eta: 0:03:46  lr: 0.002060  loss: 0.4113 (0.4599)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1218 (0.1577)  loss_objectness: 0.1178 (0.1575)  loss_rpn_box_reg: 0.1067 (0.1446)  time: 0.3776  data: 0.1204  max mem: 5197\n",
      "Training Epoch: [0]  [ 420/1000]  eta: 0:03:42  lr: 0.002110  loss: 0.4214 (0.4597)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1414 (0.1584)  loss_objectness: 0.1203 (0.1570)  loss_rpn_box_reg: 0.1115 (0.1443)  time: 0.3677  data: 0.1177  max mem: 5197\n",
      "Training Epoch: [0]  [ 430/1000]  eta: 0:03:38  lr: 0.002160  loss: 0.3901 (0.4578)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1591 (0.1589)  loss_objectness: 0.1130 (0.1558)  loss_rpn_box_reg: 0.0948 (0.1431)  time: 0.3686  data: 0.1172  max mem: 5197\n",
      "Training Epoch: [0]  [ 440/1000]  eta: 0:03:34  lr: 0.002210  loss: 0.3901 (0.4607)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1500 (0.1591)  loss_objectness: 0.1101 (0.1552)  loss_rpn_box_reg: 0.1576 (0.1464)  time: 0.3807  data: 0.1185  max mem: 5197\n",
      "Training Epoch: [0]  [ 450/1000]  eta: 0:03:30  lr: 0.002260  loss: 0.4665 (0.4611)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1184 (0.1594)  loss_objectness: 0.1243 (0.1549)  loss_rpn_box_reg: 0.1576 (0.1468)  time: 0.3774  data: 0.1200  max mem: 5197\n",
      "Training Epoch: [0]  [ 460/1000]  eta: 0:03:26  lr: 0.002310  loss: 0.4312 (0.4603)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1562 (0.1594)  loss_objectness: 0.1290 (0.1546)  loss_rpn_box_reg: 0.0749 (0.1463)  time: 0.3813  data: 0.1215  max mem: 5197\n",
      "Training Epoch: [0]  [ 470/1000]  eta: 0:03:22  lr: 0.002360  loss: 0.3699 (0.4585)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1345 (0.1586)  loss_objectness: 0.1251 (0.1537)  loss_rpn_box_reg: 0.1045 (0.1462)  time: 0.3894  data: 0.1214  max mem: 5197\n",
      "Training Epoch: [0]  [ 480/1000]  eta: 0:03:18  lr: 0.002410  loss: 0.3856 (0.4588)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1396 (0.1602)  loss_objectness: 0.1028 (0.1532)  loss_rpn_box_reg: 0.1045 (0.1454)  time: 0.3786  data: 0.1218  max mem: 5197\n",
      "Training Epoch: [0]  [ 490/1000]  eta: 0:03:14  lr: 0.002460  loss: 0.4286 (0.4587)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1939 (0.1610)  loss_objectness: 0.1118 (0.1528)  loss_rpn_box_reg: 0.0930 (0.1449)  time: 0.3687  data: 0.1225  max mem: 5197\n",
      "Training Epoch: [0]  [ 500/1000]  eta: 0:03:11  lr: 0.002510  loss: 0.4107 (0.4582)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1452 (0.1605)  loss_objectness: 0.1120 (0.1530)  loss_rpn_box_reg: 0.0930 (0.1448)  time: 0.3775  data: 0.1215  max mem: 5197\n",
      "Training Epoch: [0]  [ 510/1000]  eta: 0:03:07  lr: 0.002560  loss: 0.3625 (0.4568)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1210 (0.1603)  loss_objectness: 0.1241 (0.1524)  loss_rpn_box_reg: 0.0988 (0.1440)  time: 0.3753  data: 0.1191  max mem: 5197\n",
      "Training Epoch: [0]  [ 520/1000]  eta: 0:03:03  lr: 0.002610  loss: 0.3635 (0.4558)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1131 (0.1598)  loss_objectness: 0.1262 (0.1524)  loss_rpn_box_reg: 0.1107 (0.1437)  time: 0.3756  data: 0.1190  max mem: 5197\n",
      "Training Epoch: [0]  [ 530/1000]  eta: 0:02:59  lr: 0.002660  loss: 0.4152 (0.4556)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1391 (0.1598)  loss_objectness: 0.1309 (0.1526)  loss_rpn_box_reg: 0.1212 (0.1432)  time: 0.3819  data: 0.1241  max mem: 5197\n",
      "Training Epoch: [0]  [ 540/1000]  eta: 0:02:55  lr: 0.002710  loss: 0.4152 (0.4549)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1724 (0.1600)  loss_objectness: 0.1359 (0.1523)  loss_rpn_box_reg: 0.0955 (0.1425)  time: 0.3899  data: 0.1277  max mem: 5197\n",
      "Training Epoch: [0]  [ 550/1000]  eta: 0:02:52  lr: 0.002760  loss: 0.4007 (0.4536)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1662 (0.1595)  loss_objectness: 0.1352 (0.1524)  loss_rpn_box_reg: 0.0765 (0.1417)  time: 0.4047  data: 0.1315  max mem: 5197\n",
      "Training Epoch: [0]  [ 560/1000]  eta: 0:02:48  lr: 0.002810  loss: 0.4313 (0.4538)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1435 (0.1601)  loss_objectness: 0.1437 (0.1523)  loss_rpn_box_reg: 0.0955 (0.1414)  time: 0.3790  data: 0.1309  max mem: 5197\n",
      "Training Epoch: [0]  [ 570/1000]  eta: 0:02:44  lr: 0.002860  loss: 0.4404 (0.4529)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1628 (0.1600)  loss_objectness: 0.1433 (0.1519)  loss_rpn_box_reg: 0.0844 (0.1410)  time: 0.3709  data: 0.1244  max mem: 5197\n",
      "Training Epoch: [0]  [ 580/1000]  eta: 0:02:40  lr: 0.002910  loss: 0.3538 (0.4517)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1319 (0.1599)  loss_objectness: 0.1216 (0.1515)  loss_rpn_box_reg: 0.0780 (0.1403)  time: 0.3744  data: 0.1175  max mem: 5197\n",
      "Training Epoch: [0]  [ 590/1000]  eta: 0:02:36  lr: 0.002960  loss: 0.3899 (0.4510)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1386 (0.1603)  loss_objectness: 0.1134 (0.1510)  loss_rpn_box_reg: 0.0784 (0.1398)  time: 0.3727  data: 0.1181  max mem: 5197\n",
      "Training Epoch: [0]  [ 600/1000]  eta: 0:02:32  lr: 0.003010  loss: 0.3925 (0.4505)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1556 (0.1604)  loss_objectness: 0.0896 (0.1503)  loss_rpn_box_reg: 0.0995 (0.1399)  time: 0.3838  data: 0.1221  max mem: 5197\n",
      "Training Epoch: [0]  [ 610/1000]  eta: 0:02:28  lr: 0.003060  loss: 0.4082 (0.4520)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1668 (0.1614)  loss_objectness: 0.1138 (0.1507)  loss_rpn_box_reg: 0.1116 (0.1399)  time: 0.3671  data: 0.1228  max mem: 5197\n",
      "Training Epoch: [0]  [ 620/1000]  eta: 0:02:24  lr: 0.003110  loss: 0.4359 (0.4514)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1387 (0.1606)  loss_objectness: 0.1396 (0.1507)  loss_rpn_box_reg: 0.1195 (0.1400)  time: 0.3618  data: 0.1209  max mem: 5197\n",
      "Training Epoch: [0]  [ 630/1000]  eta: 0:02:21  lr: 0.003160  loss: 0.3804 (0.4503)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1332 (0.1605)  loss_objectness: 0.1109 (0.1499)  loss_rpn_box_reg: 0.1103 (0.1399)  time: 0.3692  data: 0.1171  max mem: 5197\n",
      "Training Epoch: [0]  [ 640/1000]  eta: 0:02:17  lr: 0.003210  loss: 0.3458 (0.4491)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1424 (0.1603)  loss_objectness: 0.1042 (0.1493)  loss_rpn_box_reg: 0.0823 (0.1395)  time: 0.3786  data: 0.1182  max mem: 5197\n",
      "Training Epoch: [0]  [ 650/1000]  eta: 0:02:13  lr: 0.003260  loss: 0.3849 (0.4494)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1377 (0.1598)  loss_objectness: 0.1312 (0.1493)  loss_rpn_box_reg: 0.0847 (0.1402)  time: 0.3843  data: 0.1214  max mem: 5197\n",
      "Training Epoch: [0]  [ 660/1000]  eta: 0:02:09  lr: 0.003310  loss: 0.4384 (0.4494)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1361 (0.1595)  loss_objectness: 0.1550 (0.1497)  loss_rpn_box_reg: 0.1266 (0.1401)  time: 0.3912  data: 0.1239  max mem: 5197\n",
      "Training Epoch: [0]  [ 670/1000]  eta: 0:02:05  lr: 0.003360  loss: 0.4384 (0.4496)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1586 (0.1598)  loss_objectness: 0.1440 (0.1497)  loss_rpn_box_reg: 0.1126 (0.1401)  time: 0.3736  data: 0.1211  max mem: 5197\n",
      "Training Epoch: [0]  [ 680/1000]  eta: 0:02:01  lr: 0.003410  loss: 0.4553 (0.4492)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1837 (0.1602)  loss_objectness: 0.1266 (0.1495)  loss_rpn_box_reg: 0.0884 (0.1395)  time: 0.3592  data: 0.1194  max mem: 5197\n",
      "Training Epoch: [0]  [ 690/1000]  eta: 0:01:58  lr: 0.003460  loss: 0.3805 (0.4482)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1264 (0.1595)  loss_objectness: 0.1136 (0.1490)  loss_rpn_box_reg: 0.0959 (0.1396)  time: 0.3787  data: 0.1201  max mem: 5197\n",
      "Training Epoch: [0]  [ 700/1000]  eta: 0:01:54  lr: 0.003510  loss: 0.4167 (0.4480)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1178 (0.1592)  loss_objectness: 0.1081 (0.1489)  loss_rpn_box_reg: 0.1317 (0.1399)  time: 0.3894  data: 0.1222  max mem: 5197\n",
      "Training Epoch: [0]  [ 710/1000]  eta: 0:01:50  lr: 0.003560  loss: 0.4260 (0.4478)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1464 (0.1594)  loss_objectness: 0.1352 (0.1488)  loss_rpn_box_reg: 0.1310 (0.1395)  time: 0.3930  data: 0.1241  max mem: 5197\n",
      "Training Epoch: [0]  [ 720/1000]  eta: 0:01:46  lr: 0.003610  loss: 0.3833 (0.4467)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1519 (0.1596)  loss_objectness: 0.1307 (0.1484)  loss_rpn_box_reg: 0.0815 (0.1387)  time: 0.3792  data: 0.1216  max mem: 5197\n",
      "Training Epoch: [0]  [ 730/1000]  eta: 0:01:42  lr: 0.003660  loss: 0.3585 (0.4452)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1618 (0.1598)  loss_objectness: 0.1022 (0.1478)  loss_rpn_box_reg: 0.0675 (0.1376)  time: 0.3753  data: 0.1182  max mem: 5197\n",
      "Training Epoch: [0]  [ 740/1000]  eta: 0:01:39  lr: 0.003710  loss: 0.3395 (0.4444)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1618 (0.1600)  loss_objectness: 0.0869 (0.1473)  loss_rpn_box_reg: 0.0652 (0.1370)  time: 0.3815  data: 0.1161  max mem: 5197\n",
      "Training Epoch: [0]  [ 750/1000]  eta: 0:01:35  lr: 0.003760  loss: 0.3663 (0.4442)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1451 (0.1596)  loss_objectness: 0.1263 (0.1481)  loss_rpn_box_reg: 0.0799 (0.1365)  time: 0.3908  data: 0.1187  max mem: 5197\n",
      "Training Epoch: [0]  [ 760/1000]  eta: 0:01:31  lr: 0.003810  loss: 0.3901 (0.4433)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1246 (0.1592)  loss_objectness: 0.1620 (0.1480)  loss_rpn_box_reg: 0.0900 (0.1361)  time: 0.3997  data: 0.1238  max mem: 5197\n",
      "Training Epoch: [0]  [ 770/1000]  eta: 0:01:27  lr: 0.003860  loss: 0.3994 (0.4429)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1244 (0.1589)  loss_objectness: 0.1435 (0.1480)  loss_rpn_box_reg: 0.0964 (0.1360)  time: 0.3902  data: 0.1242  max mem: 5197\n",
      "Training Epoch: [0]  [ 780/1000]  eta: 0:01:24  lr: 0.003910  loss: 0.3407 (0.4413)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1102 (0.1584)  loss_objectness: 0.1269 (0.1476)  loss_rpn_box_reg: 0.0907 (0.1353)  time: 0.3900  data: 0.1198  max mem: 5197\n",
      "Training Epoch: [0]  [ 790/1000]  eta: 0:01:20  lr: 0.003960  loss: 0.3958 (0.4414)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1162 (0.1585)  loss_objectness: 0.1130 (0.1474)  loss_rpn_box_reg: 0.0854 (0.1356)  time: 0.3807  data: 0.1176  max mem: 5197\n",
      "Training Epoch: [0]  [ 800/1000]  eta: 0:01:16  lr: 0.004010  loss: 0.4061 (0.4407)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1540 (0.1583)  loss_objectness: 0.1292 (0.1472)  loss_rpn_box_reg: 0.0921 (0.1352)  time: 0.3661  data: 0.1216  max mem: 5197\n",
      "Training Epoch: [0]  [ 810/1000]  eta: 0:01:12  lr: 0.004060  loss: 0.4061 (0.4408)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1540 (0.1583)  loss_objectness: 0.1487 (0.1476)  loss_rpn_box_reg: 0.0973 (0.1349)  time: 0.3788  data: 0.1273  max mem: 5197\n",
      "Training Epoch: [0]  [ 820/1000]  eta: 0:01:08  lr: 0.004110  loss: 0.4280 (0.4414)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1598 (0.1582)  loss_objectness: 0.1598 (0.1477)  loss_rpn_box_reg: 0.0973 (0.1355)  time: 0.3910  data: 0.1254  max mem: 5197\n",
      "Training Epoch: [0]  [ 830/1000]  eta: 0:01:04  lr: 0.004160  loss: 0.4008 (0.4417)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1421 (0.1584)  loss_objectness: 0.1598 (0.1480)  loss_rpn_box_reg: 0.1329 (0.1353)  time: 0.3945  data: 0.1216  max mem: 5197\n",
      "Training Epoch: [0]  [ 840/1000]  eta: 0:01:01  lr: 0.004210  loss: 0.4422 (0.4421)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1147 (0.1583)  loss_objectness: 0.1511 (0.1479)  loss_rpn_box_reg: 0.1450 (0.1359)  time: 0.3878  data: 0.1197  max mem: 5197\n",
      "Training Epoch: [0]  [ 850/1000]  eta: 0:00:57  lr: 0.004260  loss: 0.4065 (0.4418)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1195 (0.1586)  loss_objectness: 0.1364 (0.1475)  loss_rpn_box_reg: 0.1364 (0.1357)  time: 0.3784  data: 0.1189  max mem: 5197\n",
      "Training Epoch: [0]  [ 860/1000]  eta: 0:00:53  lr: 0.004310  loss: 0.3870 (0.4413)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1629 (0.1587)  loss_objectness: 0.1010 (0.1475)  loss_rpn_box_reg: 0.0881 (0.1351)  time: 0.3756  data: 0.1185  max mem: 5197\n",
      "Training Epoch: [0]  [ 870/1000]  eta: 0:00:49  lr: 0.004360  loss: 0.4117 (0.4422)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1687 (0.1592)  loss_objectness: 0.1422 (0.1475)  loss_rpn_box_reg: 0.1086 (0.1355)  time: 0.3629  data: 0.1186  max mem: 5197\n",
      "Training Epoch: [0]  [ 880/1000]  eta: 0:00:45  lr: 0.004410  loss: 0.3964 (0.4416)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1613 (0.1588)  loss_objectness: 0.1523 (0.1475)  loss_rpn_box_reg: 0.1185 (0.1353)  time: 0.3661  data: 0.1199  max mem: 5197\n",
      "Training Epoch: [0]  [ 890/1000]  eta: 0:00:41  lr: 0.004460  loss: 0.4235 (0.4418)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1397 (0.1588)  loss_objectness: 0.1359 (0.1475)  loss_rpn_box_reg: 0.1074 (0.1354)  time: 0.3840  data: 0.1227  max mem: 5197\n",
      "Training Epoch: [0]  [ 900/1000]  eta: 0:00:38  lr: 0.004510  loss: 0.4279 (0.4415)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1429 (0.1586)  loss_objectness: 0.1270 (0.1474)  loss_rpn_box_reg: 0.1185 (0.1355)  time: 0.3973  data: 0.1250  max mem: 5197\n",
      "Training Epoch: [0]  [ 910/1000]  eta: 0:00:34  lr: 0.004560  loss: 0.3978 (0.4414)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1553 (0.1589)  loss_objectness: 0.1245 (0.1472)  loss_rpn_box_reg: 0.0911 (0.1353)  time: 0.3914  data: 0.1234  max mem: 5197\n",
      "Training Epoch: [0]  [ 920/1000]  eta: 0:00:30  lr: 0.004610  loss: 0.4187 (0.4412)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1822 (0.1591)  loss_objectness: 0.1127 (0.1469)  loss_rpn_box_reg: 0.0911 (0.1352)  time: 0.3783  data: 0.1226  max mem: 5197\n",
      "Training Epoch: [0]  [ 930/1000]  eta: 0:00:26  lr: 0.004660  loss: 0.4371 (0.4414)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1638 (0.1590)  loss_objectness: 0.1228 (0.1470)  loss_rpn_box_reg: 0.1069 (0.1354)  time: 0.3895  data: 0.1249  max mem: 5197\n",
      "Training Epoch: [0]  [ 940/1000]  eta: 0:00:22  lr: 0.004710  loss: 0.4427 (0.4414)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1658 (0.1592)  loss_objectness: 0.1224 (0.1468)  loss_rpn_box_reg: 0.1201 (0.1354)  time: 0.3959  data: 0.1253  max mem: 5197\n",
      "Training Epoch: [0]  [ 950/1000]  eta: 0:00:19  lr: 0.004760  loss: 0.4146 (0.4410)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1613 (0.1592)  loss_objectness: 0.1034 (0.1466)  loss_rpn_box_reg: 0.0859 (0.1352)  time: 0.3836  data: 0.1226  max mem: 5197\n",
      "Training Epoch: [0]  [ 960/1000]  eta: 0:00:15  lr: 0.004810  loss: 0.4343 (0.4413)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1658 (0.1594)  loss_objectness: 0.1036 (0.1465)  loss_rpn_box_reg: 0.1442 (0.1355)  time: 0.3731  data: 0.1234  max mem: 5197\n",
      "Training Epoch: [0]  [ 970/1000]  eta: 0:00:11  lr: 0.004860  loss: 0.4453 (0.4423)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1710 (0.1592)  loss_objectness: 0.1335 (0.1468)  loss_rpn_box_reg: 0.1545 (0.1363)  time: 0.3685  data: 0.1220  max mem: 5197\n",
      "Training Epoch: [0]  [ 980/1000]  eta: 0:00:07  lr: 0.004910  loss: 0.5186 (0.4427)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1503 (0.1593)  loss_objectness: 0.1272 (0.1468)  loss_rpn_box_reg: 0.1558 (0.1366)  time: 0.3650  data: 0.1224  max mem: 5197\n",
      "Training Epoch: [0]  [ 990/1000]  eta: 0:00:03  lr: 0.004960  loss: 0.4937 (0.4428)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1073 (0.1587)  loss_objectness: 0.1301 (0.1468)  loss_rpn_box_reg: 0.1873 (0.1373)  time: 0.3704  data: 0.1220  max mem: 5197\n",
      "Training Epoch: [0]  [ 999/1000]  eta: 0:00:00  lr: 0.005000  loss: 0.4604 (0.4429)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1343 (0.1588)  loss_objectness: 0.1316 (0.1471)  loss_rpn_box_reg: 0.1145 (0.1370)  time: 0.3728  data: 0.1210  max mem: 5197\n",
      "Training Epoch: [0] Total time: 0:06:21 (0.3814 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/250]  eta: 0:02:16  model_time: 0.4171 (0.4171)  evaluator_time: 0.0190 (0.0190)  time: 0.5441  data: 0.1050  max mem: 5197\n",
      "Test:  [100/250]  eta: 0:00:45  model_time: 0.1750 (0.1579)  evaluator_time: 0.0200 (0.0226)  time: 0.3137  data: 0.1266  max mem: 5197\n",
      "Test:  [200/250]  eta: 0:00:14  model_time: 0.1500 (0.1553)  evaluator_time: 0.0220 (0.0223)  time: 0.2786  data: 0.1076  max mem: 5197\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE='CLIP-FRCNN'\n",
    "# CLIP-Backbone-FRCNN creates a FRCNN using CLIP features as the model backbone\n",
    "# CLIP-FRCNN creates a FRCNN using CLIP features as the model backbone, and embeds the rois using CLIP's embedding\n",
    "# Fully custom vanilla uses a pre-trained resnet50 backbone, and generates new anchor generator and roi pooling\n",
    "# Custom-Vanilla uses the pre-trained FRCNN from pytorch and replaces the roi heads only\n",
    "#\n",
    "import clip\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in item_list]).cuda()\n",
    "\n",
    "model = create_model(MODEL_TYPE, text_tokens)\n",
    "test = False\n",
    "\n",
    "\n",
    "if test:\n",
    "    train_model(model, evaluation_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=4)\n",
    "else:\n",
    "    train_model(model, train_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE='CLIP-Backbone-FRCNN'\n",
    "\n",
    "model = create_model(MODEL_TYPE, classes=item_list)\n",
    "test = False\n",
    "\n",
    "if test:\n",
    "    train_model(model, evaluation_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=2)\n",
    "else:\n",
    "    train_model(model, train_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "acbb3df601244291b8b2fb9ea1137573",
      "32b6ec3046e64d04b4134553dc434fe0",
      "d8c6a316609d4ca5bfee139b93177ef5",
      "a1645bdfb02b42fba268f7000f183639",
      "4a4788a4fd6841788b20cfbf54a3d10b",
      "5d836b94d13e459d82429606496e4d4f",
      "a410071b34034a91aeda7ef1114969c2",
      "c063e7d90f6a4027b53d1b70c8c07742"
     ]
    },
    "id": "bHa6KRbEWuxz",
    "outputId": "3b4ebd0b-aa69-4a4d-b73c-b8cf24d8b461"
   },
   "outputs": [],
   "source": [
    "#train a custom vanilla model so that we can compare and make sure the CLIP FRCNN is comparable\n",
    "# Fully-Custom-Vanilla is most appropriate as it generates the model in a similar fashion\n",
    "MODEL_TYPE = 'Fully-Custom-Vanilla'\n",
    "\n",
    "vanilla_model = create_model(MODEL_TYPE, classes=item_list)\n",
    "train_model(vanilla_model, train_dataset, evaluation_dataset, num_epochs=10, MODEL_TYPE=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj3vLT1eXFnk"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkzG1i3AW1O7",
    "outputId": "ec7971c3-66ef-4a57-e710-248cb53dee8e"
   },
   "outputs": [],
   "source": [
    "add_detections(model, evaluation_dataset, fo_dataset, field_name=\"predictions\")\n",
    "\n",
    "results = fo.evaluate_detections(\n",
    "    test_view,\n",
    "    \"predictions\",\n",
    "    classes=item_list,\n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uYdXrhgYdJ_",
    "outputId": "2eb792e9-342f-4dc8-f6c0-e1503d8bf193"
   },
   "outputs": [],
   "source": [
    "results.mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcJBOM76aJPR",
    "outputId": "ac452527-7608-4e8d-f57e-18a0470acd30"
   },
   "outputs": [],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nddFfGSnXo7i"
   },
   "source": [
    "By default, objects are only matched with other objects of the same class. In order to get an interesting confusion matrix, we need to match interclass objects by setting `classwise=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_53aCMna2Vt",
    "outputId": "4db71f31-73e3-4036-f623-efd8e2ac85bf"
   },
   "outputs": [],
   "source": [
    "results_interclass = fo.evaluate_detections(\n",
    "    test_view, \n",
    "    \"predictions\", \n",
    "    classes=item_list,\n",
    "    compute_mAP=True, \n",
    "    classwise=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot = results.plot_pr_curves(classes=item_list)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Nbqf-NuAZ7Ps",
    "outputId": "571cd947-c94a-4b9a-ed93-2330fbddea7e"
   },
   "outputs": [],
   "source": [
    "results_interclass.plot_confusion_matrix(classes=item_list, include_other=False, include_missing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElSV7tTbYKLr"
   },
   "source": [
    "The [detection evaluation](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html#detections) also added the attributes `eval_fp`, `eval_tp`, and `eval_fn` to every predicted detection indicating if it is a false positive, true positive, or false negative. \n",
    "Let's create a view to find the worst samples by sorting by `eval_fp` using the [FiftyOne App](https://voxel51.com/docs/fiftyone/user_guide/app.html) to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786,
     "resources": {
      "https://localhost:5151/polling?sessionId=de0b710e-15f8-4c57-ba46-ae7955f716b1": {
       "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
       "headers": [
        [
         "access-control-allow-headers",
         "x-requested-with"
        ],
        [
         "content-type",
         "text/html; charset=UTF-8"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "Pm4Z52rd8AC1",
    "outputId": "62d39076-7ef3-4fe3-95ae-500d0f8f8a3f"
   },
   "outputs": [],
   "source": [
    "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786,
     "resources": {
      "https://localhost:5151/polling?sessionId=ebbc318d-3578-4fb1-9ae7-68596117572b": {
       "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
       "headers": [
        [
         "access-control-allow-headers",
         "x-requested-with"
        ],
        [
         "content-type",
         "text/html; charset=UTF-8"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "njLG0l5K-ucV",
    "outputId": "bda6f02d-d8fe-49be-d212-31e0e70779e3"
   },
   "outputs": [],
   "source": [
    "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReXDVFgLZLtf"
   },
   "source": [
    "It would be best to get this [data reannotated to fix these mistakes](https://towardsdatascience.com/managing-annotation-mistakes-with-fiftyone-and-labelbox-fc6e87b51102), but in the meantime, we can easily remedy this by simply creating a new view that remaps the labels `car`, `truck`, and `bus` all to `vehicle` and then retraining the model with that. This is only possible because we are backing our data in FiftyOne and loading views into PyTorch as needed. Without FiftyOne, the PyTorch dataset class or the underlying data would need to be changed to remap these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# map labels to single vehicle class\n",
    "vehicle_list = ['car', 'bus', 'truck']\n",
    "vehicles_map = {c: \"vehicle\" for c in vehicle_list}\n",
    "\n",
    "train_map_view = train_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "test_map_view = test_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "torch_map_dataset = FiftyOneTorchDataset(train_map_view, train_transforms)\n",
    "torch_map_dataset_test = FiftyOneTorchDataset(test_map_view, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynRCHQv8XB_v"
   },
   "outputs": [],
   "source": [
    "# Only 2 classes (background and vehicle)\n",
    "MODEL_TYPE = 'Vanilla-FRCNN'\n",
    "vehicle_model = create_model(MODEL_TYPE, num_classes=(len(vehicles_map)+1))\n",
    "train_model(vehicle_model, torch_map_dataset, torch_map_dataset_test, num_epochs=2, MODEL_TYPE=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-mrVOl4XFbp",
    "outputId": "6d8bec76-ebe8-4a36-959a-52bb1aab8498"
   },
   "outputs": [],
   "source": [
    "add_detections(vehicle_model, torch_map_dataset_test, test_map_view, field_name=\"vehicle_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfd3xvhaXhl_",
    "outputId": "d9c4a2fe-538a-4979-c3f8-f5ede0c98aa1"
   },
   "outputs": [],
   "source": [
    "vehicle_results = fo.evaluate_detections(\n",
    "    test_map_view, \n",
    "    \"vehicle_predictions\", \n",
    "    classes=[\"vehicle\"], \n",
    "    eval_key=\"vehicle_eval\", \n",
    "    compute_mAP=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFvddH3rk0NR",
    "outputId": "59572ba2-f9ad-4dd2-e9ac-90877190ff99"
   },
   "outputs": [],
   "source": [
    "vehicle_results.mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwbhq18sk1PL",
    "outputId": "d6985867-5049-4678-cc88-d5041a0079ed"
   },
   "outputs": [],
   "source": [
    "vehicle_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJMAkJbWZ_u1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Due to our ability to easily visualize and manage our dataset with FiftyOne, we were able to spot and take action on a dataset issue that would otherwise have gone unnoticed if we only concerned ourselves with dataset-wide evaluation metrics and fixed dataset representations. Through these efforts, we managed to increase the mAP of the model to 43%.\n",
    "\n",
    "Even though this example workflow may not work in all situations, this kind of class-merging strategy can be effective in cases where more fine-grained discrimination is not called for."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fiftyone_pytorch_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "torch-frcnn",
   "language": "python",
   "display_name": "torch-frcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32b6ec3046e64d04b4134553dc434fe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a4788a4fd6841788b20cfbf54a3d10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d836b94d13e459d82429606496e4d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1645bdfb02b42fba268f7000f183639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c063e7d90f6a4027b53d1b70c8c07742",
      "placeholder": "​",
      "style": "IPY_MODEL_a410071b34034a91aeda7ef1114969c2",
      "value": " 160M/160M [01:05&lt;00:00, 2.55MB/s]"
     }
    },
    "a410071b34034a91aeda7ef1114969c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acbb3df601244291b8b2fb9ea1137573": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8c6a316609d4ca5bfee139b93177ef5",
       "IPY_MODEL_a1645bdfb02b42fba268f7000f183639"
      ],
      "layout": "IPY_MODEL_32b6ec3046e64d04b4134553dc434fe0"
     }
    },
    "c063e7d90f6a4027b53d1b70c8c07742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8c6a316609d4ca5bfee139b93177ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d836b94d13e459d82429606496e4d4f",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a4788a4fd6841788b20cfbf54a3d10b",
      "value": 167502836
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}