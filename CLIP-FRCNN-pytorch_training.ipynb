{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Code for using FiftyOne to train a Faster RCNN on COCO data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1682a69b8d0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import torchvision.models.detection.roi_heads\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "from dataset import FiftyOneTorchDataset, get_transforms\n",
    "from model import create_model\n",
    "from utils import add_detections\n",
    "\n",
    "from engine import train_model\n",
    "import config\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full dataset from model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5crNDNsRWdPT",
    "outputId": "4f3ff734-ca0a-4312-a811-7f84db514fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\blain\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\blain\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [14.7s elapsed, 0s remaining, 348.5 samples/s]      \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x16866726af0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=8981b223-0528-4299-8e69-b623750c6bc9\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lodad in the dataset from the FiftyOne model Zoo\n",
    "fo_dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")\n",
    "\n",
    "#needed to calculate image height and width\n",
    "fo_dataset.compute_metadata()\n",
    "\n",
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqU6Ckq4WKHK"
   },
   "source": [
    "For example, cluttered images make it difficult for models to localize objects. We can use FiftyOne to create a view containing only samples with more than, say, 10 objects. You can perform the same operations on views as datasets, so we can create an instance of our PyTorch dataset from this view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kLACOukJFUxd"
   },
   "outputs": [],
   "source": [
    "#if we want to see images with more than 10 items, we can\n",
    "# busy_view = fo_dataset.match(F(\"ground_truth.detections\").length() > 10)\n",
    "# busy_torch_dataset = FiftyOneTorchDataset(busy_view)\n",
    "# session.view = busy_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKsE_7TOWXBE"
   },
   "source": [
    "### Create training and testing views (and corresponding PyTorch datasets) that only contain some items from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TELK0NWmWrMT",
    "outputId": "8bf582cf-e483-4643-8f6b-7c664a2d6c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning on 4000 samples\n",
      "Testing on 1000 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x16860d78250>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=21851f65-376a-4121-9514-bc175ce8c444\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset = False\n",
    "\n",
    "train_transforms, test_transforms = get_transforms()\n",
    "\n",
    "if subset:\n",
    "    # to filter certain items from the dataset we can\n",
    "    item_list = [\"car\", \"dog\", \"bus\", 'fork', 'airplane']\n",
    "    #item_list = ['airplane', 'cat']\n",
    "    item_view = fo_dataset.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(item_list))\n",
    "\n",
    "    #session.view = item_view\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    train_view = item_view.take((len(item_view) * config.TRAIN_TEST_SPLIT), seed=51)\n",
    "    test_view = item_view.exclude([s.id for s in train_view])\n",
    "\n",
    "else:\n",
    "    train_view = fo_dataset.take(len(fo_dataset) * config.TRAIN_TEST_SPLIT)\n",
    "    test_view = fo_dataset.exclude([s.id for s in train_view])\n",
    "    item_list = fo_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "print(f'Traning on {len(train_view)} samples')\n",
    "print(f'Testing on {len(test_view)} samples')\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "train_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
    "        classes=item_list)\n",
    "evaluation_dataset = FiftyOneTorchDataset(test_view, test_transforms,\n",
    "        classes=item_list)\n",
    "\n",
    "session.view = train_view\n",
    "\n",
    "#this is needed for later use, but not for creating the dataset\n",
    "if item_list[0] != 'background':\n",
    "     item_list.insert(0,'background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # map labels to single vehicle class\n",
    "# vehicle_list = ['car', 'bus', 'truck']\n",
    "# vehicles_map = {c: \"vehicle\" for c in vehicle_list}\n",
    "#\n",
    "# train_map_view = train_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "# test_map_view = test_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "#\n",
    "# # use our dataset and defined transformations\n",
    "# torch_map_dataset = FiftyOneTorchDataset(train_map_view, train_transforms)\n",
    "# torch_map_dataset_test = FiftyOneTorchDataset(test_map_view, test_transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5je6lVBWz5r"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpn nms thresh: 0.7\n",
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blain\\anaconda3\\envs\\torch-frcnn\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: [0]  [  0/250]  eta: 0:06:25  lr: 0.000002  loss: 0.9180 (0.9180)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0041 (0.0041)  loss_objectness: 0.7379 (0.7379)  loss_rpn_box_reg: 0.1760 (0.1760)  time: 1.5427  data: 0.4882  max mem: 3291\n",
      "Training Epoch: [0]  [ 10/250]  eta: 0:03:45  lr: 0.000014  loss: 0.8340 (0.8320)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0040 (0.0039)  loss_objectness: 0.7054 (0.6972)  loss_rpn_box_reg: 0.1247 (0.1308)  time: 0.9413  data: 0.3654  max mem: 4237\n",
      "Training Epoch: [0]  [ 20/250]  eta: 0:03:26  lr: 0.000026  loss: 0.6704 (0.7111)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0023 (0.0027)  loss_objectness: 0.5811 (0.5907)  loss_rpn_box_reg: 0.1100 (0.1176)  time: 0.8639  data: 0.3475  max mem: 4237\n",
      "Training Epoch: [0]  [ 30/250]  eta: 0:03:12  lr: 0.000038  loss: 0.5324 (0.6606)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.0018 (0.0345)  loss_objectness: 0.3646 (0.5056)  loss_rpn_box_reg: 0.1104 (0.1204)  time: 0.8364  data: 0.3488  max mem: 4237\n",
      "Training Epoch: [0]  [ 40/250]  eta: 0:03:02  lr: 0.000050  loss: 0.5772 (0.6433)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.1280 (0.0740)  loss_objectness: 0.2979 (0.4460)  loss_rpn_box_reg: 0.1286 (0.1233)  time: 0.8386  data: 0.3477  max mem: 4237\n",
      "Training Epoch: [0]  [ 50/250]  eta: 0:02:52  lr: 0.000062  loss: 0.6172 (0.6777)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.2385 (0.1478)  loss_objectness: 0.2412 (0.4066)  loss_rpn_box_reg: 0.1286 (0.1233)  time: 0.8475  data: 0.3412  max mem: 4317\n",
      "Training Epoch: [0]  [ 60/250]  eta: 0:02:42  lr: 0.000074  loss: 0.6848 (0.6923)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.3781 (0.1883)  loss_objectness: 0.2409 (0.3796)  loss_rpn_box_reg: 0.1246 (0.1244)  time: 0.8265  data: 0.3453  max mem: 4317\n",
      "Training Epoch: [0]  [ 70/250]  eta: 0:02:33  lr: 0.000086  loss: 0.6848 (0.7085)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.3503 (0.2236)  loss_objectness: 0.2226 (0.3559)  loss_rpn_box_reg: 0.1321 (0.1291)  time: 0.8182  data: 0.3441  max mem: 4317\n",
      "Training Epoch: [0]  [ 80/250]  eta: 0:02:24  lr: 0.000098  loss: 0.8395 (0.7234)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4389 (0.2550)  loss_objectness: 0.2088 (0.3378)  loss_rpn_box_reg: 0.1470 (0.1305)  time: 0.8313  data: 0.3404  max mem: 4317\n",
      "Training Epoch: [0]  [ 90/250]  eta: 0:02:15  lr: 0.000110  loss: 0.7800 (0.7209)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4424 (0.2705)  loss_objectness: 0.1926 (0.3222)  loss_rpn_box_reg: 0.1173 (0.1282)  time: 0.8423  data: 0.3440  max mem: 4317\n",
      "Training Epoch: [0]  [100/250]  eta: 0:02:07  lr: 0.000122  loss: 0.7329 (0.7294)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4204 (0.2897)  loss_objectness: 0.2027 (0.3115)  loss_rpn_box_reg: 0.1150 (0.1282)  time: 0.8405  data: 0.3469  max mem: 4317\n",
      "Training Epoch: [0]  [110/250]  eta: 0:01:58  lr: 0.000134  loss: 0.7692 (0.7355)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4644 (0.3072)  loss_objectness: 0.2027 (0.3006)  loss_rpn_box_reg: 0.1165 (0.1277)  time: 0.8271  data: 0.3431  max mem: 4317\n",
      "Training Epoch: [0]  [120/250]  eta: 0:01:50  lr: 0.000146  loss: 0.8612 (0.7452)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4903 (0.3274)  loss_objectness: 0.1832 (0.2911)  loss_rpn_box_reg: 0.1130 (0.1267)  time: 0.8437  data: 0.3415  max mem: 4317\n",
      "Training Epoch: [0]  [130/250]  eta: 0:01:41  lr: 0.000158  loss: 0.8742 (0.7537)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5483 (0.3454)  loss_objectness: 0.1875 (0.2830)  loss_rpn_box_reg: 0.1044 (0.1253)  time: 0.8474  data: 0.3429  max mem: 4317\n",
      "Training Epoch: [0]  [140/250]  eta: 0:01:33  lr: 0.000170  loss: 0.9226 (0.7760)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.6689 (0.3753)  loss_objectness: 0.1837 (0.2753)  loss_rpn_box_reg: 0.1180 (0.1254)  time: 0.8533  data: 0.3395  max mem: 4317\n",
      "Training Epoch: [0]  [150/250]  eta: 0:01:24  lr: 0.000182  loss: 1.0273 (0.7840)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.7270 (0.3893)  loss_objectness: 0.1832 (0.2694)  loss_rpn_box_reg: 0.1230 (0.1252)  time: 0.8697  data: 0.3403  max mem: 4317\n",
      "Training Epoch: [0]  [160/250]  eta: 0:01:16  lr: 0.000194  loss: 0.8209 (0.7858)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5220 (0.3971)  loss_objectness: 0.1877 (0.2640)  loss_rpn_box_reg: 0.1142 (0.1246)  time: 0.8552  data: 0.3463  max mem: 4317\n",
      "Training Epoch: [0]  [170/250]  eta: 0:01:08  lr: 0.000206  loss: 0.8208 (0.7907)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5423 (0.4072)  loss_objectness: 0.1822 (0.2589)  loss_rpn_box_reg: 0.1142 (0.1246)  time: 0.8776  data: 0.3461  max mem: 4317\n",
      "Training Epoch: [0]  [180/250]  eta: 0:00:59  lr: 0.000218  loss: 0.8574 (0.7950)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.6031 (0.4164)  loss_objectness: 0.1678 (0.2546)  loss_rpn_box_reg: 0.1086 (0.1240)  time: 0.8998  data: 0.3506  max mem: 4324\n",
      "Training Epoch: [0]  [190/250]  eta: 0:00:51  lr: 0.000230  loss: 0.9268 (0.8070)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.6435 (0.4335)  loss_objectness: 0.1724 (0.2507)  loss_rpn_box_reg: 0.0918 (0.1228)  time: 0.8971  data: 0.3588  max mem: 4324\n",
      "Training Epoch: [0]  [200/250]  eta: 0:00:42  lr: 0.000242  loss: 0.9268 (0.8054)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5993 (0.4356)  loss_objectness: 0.1844 (0.2478)  loss_rpn_box_reg: 0.0918 (0.1219)  time: 0.8879  data: 0.3562  max mem: 4324\n",
      "Training Epoch: [0]  [210/250]  eta: 0:00:34  lr: 0.000254  loss: 0.6557 (0.7987)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.3486 (0.4320)  loss_objectness: 0.1844 (0.2454)  loss_rpn_box_reg: 0.0934 (0.1214)  time: 0.8467  data: 0.3470  max mem: 4324\n",
      "Training Epoch: [0]  [220/250]  eta: 0:00:25  lr: 0.000266  loss: 0.7141 (0.7995)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4050 (0.4362)  loss_objectness: 0.1861 (0.2426)  loss_rpn_box_reg: 0.1055 (0.1207)  time: 0.8441  data: 0.3444  max mem: 4324\n",
      "Training Epoch: [0]  [230/250]  eta: 0:00:17  lr: 0.000278  loss: 0.8746 (0.8071)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5964 (0.4486)  loss_objectness: 0.1670 (0.2390)  loss_rpn_box_reg: 0.1037 (0.1195)  time: 0.8624  data: 0.3443  max mem: 4324\n",
      "Training Epoch: [0]  [240/250]  eta: 0:00:08  lr: 0.000290  loss: 0.8520 (0.8057)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5869 (0.4503)  loss_objectness: 0.1684 (0.2366)  loss_rpn_box_reg: 0.1017 (0.1188)  time: 0.8500  data: 0.3433  max mem: 4324\n",
      "Training Epoch: [0]  [249/250]  eta: 0:00:00  lr: 0.000300  loss: 0.8520 (0.8115)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5869 (0.4594)  loss_objectness: 0.1762 (0.2343)  loss_rpn_box_reg: 0.1024 (0.1178)  time: 0.8567  data: 0.3441  max mem: 4324\n",
      "Training Epoch: [0] Total time: 0:03:34 (0.8561 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/62]  eta: 0:00:52  model_time: 0.5331 (0.5331)  evaluator_time: 0.0200 (0.0200)  time: 0.8462  data: 0.2851  max mem: 4324\n",
      "Test:  [61/62]  eta: 0:00:00  model_time: 0.2981 (0.2976)  evaluator_time: 0.0190 (0.0207)  time: 0.6082  data: 0.2923  max mem: 4326\n",
      "Test: Total time: 0:00:38 (0.6200 s / it)\n",
      "Averaged stats: model_time: 0.2981 (0.2976)  evaluator_time: 0.0190 (0.0207)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.50s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.038\n",
      "Testing Epoch: [0]  [ 0/62]  eta: 0:00:45  lr: 0.000300  loss: 0.7268 (0.7268)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4762 (0.4762)  loss_objectness: 0.1660 (0.1660)  loss_rpn_box_reg: 0.0846 (0.0846)  time: 0.7392  data: 0.2681  max mem: 4326\n",
      "Testing Epoch: [0]  [61/62]  eta: 0:00:00  lr: 0.000300  loss: 0.9015 (0.9205)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.6441 (0.6868)  loss_objectness: 0.1508 (0.1484)  loss_rpn_box_reg: 0.0791 (0.0853)  time: 0.7205  data: 0.2951  max mem: 4531\n",
      "Testing Epoch: [0] Total time: 0:00:46 (0.7428 s / it)\n",
      "Training Epoch: [1]  [  0/250]  eta: 0:03:49  lr: 0.000300  loss: 1.0013 (1.0013)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.7371 (0.7371)  loss_objectness: 0.1784 (0.1784)  loss_rpn_box_reg: 0.0858 (0.0858)  time: 0.9182  data: 0.2721  max mem: 4531\n",
      "Training Epoch: [1]  [ 10/250]  eta: 0:03:14  lr: 0.000300  loss: 0.8435 (0.8125)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5895 (0.5770)  loss_objectness: 0.1634 (0.1574)  loss_rpn_box_reg: 0.0722 (0.0780)  time: 0.8105  data: 0.2742  max mem: 4531\n",
      "Training Epoch: [1]  [ 20/250]  eta: 0:03:01  lr: 0.000300  loss: 0.7445 (0.7991)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5129 (0.5467)  loss_objectness: 0.1634 (0.1637)  loss_rpn_box_reg: 0.0751 (0.0887)  time: 0.7825  data: 0.2750  max mem: 4531\n",
      "Training Epoch: [1]  [ 30/250]  eta: 0:02:51  lr: 0.000300  loss: 0.8143 (0.8268)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5690 (0.5696)  loss_objectness: 0.1726 (0.1661)  loss_rpn_box_reg: 0.0929 (0.0910)  time: 0.7614  data: 0.2768  max mem: 4531\n",
      "Training Epoch: [1]  [ 40/250]  eta: 0:02:43  lr: 0.000300  loss: 0.8143 (0.8275)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5690 (0.5713)  loss_objectness: 0.1603 (0.1663)  loss_rpn_box_reg: 0.0885 (0.0898)  time: 0.7721  data: 0.2750  max mem: 4531\n",
      "Training Epoch: [1]  [ 50/250]  eta: 0:02:35  lr: 0.000300  loss: 0.6495 (0.7964)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4401 (0.5461)  loss_objectness: 0.1603 (0.1636)  loss_rpn_box_reg: 0.0726 (0.0867)  time: 0.7816  data: 0.2733  max mem: 4531\n",
      "Training Epoch: [1]  [ 60/250]  eta: 0:02:28  lr: 0.000300  loss: 0.6975 (0.8011)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.4226 (0.5514)  loss_objectness: 0.1579 (0.1641)  loss_rpn_box_reg: 0.0770 (0.0857)  time: 0.7770  data: 0.2745  max mem: 4531\n",
      "Training Epoch: [1]  [ 70/250]  eta: 0:02:20  lr: 0.000300  loss: 0.8243 (0.8163)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.5469 (0.5671)  loss_objectness: 0.1579 (0.1638)  loss_rpn_box_reg: 0.0800 (0.0855)  time: 0.7742  data: 0.2735  max mem: 4531\n",
      "Training Epoch: [1]  [ 80/250]  eta: 0:02:12  lr: 0.000300  loss: 0.9187 (0.8319)  loss_classifier: 0.0000 (0.0000)  loss_box_reg: 0.6601 (0.5813)  loss_objectness: 0.1636 (0.1643)  loss_rpn_box_reg: 0.0871 (0.0863)  time: 0.7878  data: 0.2740  max mem: 4531\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE='CLIP-FRCNN'\n",
    "# CLIP-Backbone-FRCNN creates a FRCNN using CLIP features as the model backbone\n",
    "# CLIP-FRCNN creates a FRCNN using CLIP features as the model backbone, and embeds the rois using CLIP's embedding\n",
    "# Fully custom vanilla uses a pre-trained resnet50 backbone, and generates new anchor generator and roi pooling\n",
    "# Custom-Vanilla uses the pre-trained FRCNN from pytorch and replaces the roi heads only\n",
    "#\n",
    "import clip\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in item_list]).cuda()\n",
    "\n",
    "model = create_model(MODEL_TYPE, text_tokens)\n",
    "test = False\n",
    "\n",
    "print(f'rpn nms thresh: {model.rpn.nms_thresh}')\n",
    "\n",
    "if test:\n",
    "    train_model(model, evaluation_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=2)\n",
    "else:\n",
    "    train_model(model, train_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=16)\n",
    "\n",
    "#started at 1200 on 3 March 2022"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE='CLIP-Backbone-FRCNN'\n",
    "\n",
    "model = create_model(MODEL_TYPE, classes=item_list)\n",
    "test = True\n",
    "\n",
    "if test:\n",
    "    train_model(model, evaluation_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=2)\n",
    "else:\n",
    "    train_model(model, train_dataset, evaluation_dataset, num_epochs=config.NUM_EPOCHS, MODEL_TYPE=MODEL_TYPE, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "acbb3df601244291b8b2fb9ea1137573",
      "32b6ec3046e64d04b4134553dc434fe0",
      "d8c6a316609d4ca5bfee139b93177ef5",
      "a1645bdfb02b42fba268f7000f183639",
      "4a4788a4fd6841788b20cfbf54a3d10b",
      "5d836b94d13e459d82429606496e4d4f",
      "a410071b34034a91aeda7ef1114969c2",
      "c063e7d90f6a4027b53d1b70c8c07742"
     ]
    },
    "id": "bHa6KRbEWuxz",
    "outputId": "3b4ebd0b-aa69-4a4d-b73c-b8cf24d8b461",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#train a custom vanilla model so that we can compare and make sure the CLIP FRCNN is comparable\n",
    "# Fully-Custom-Vanilla is most appropriate as it generates the model in a similar fashion\n",
    "MODEL_TYPE = 'Fully-Custom-Vanilla'\n",
    "\n",
    "vanilla_model = create_model(MODEL_TYPE, classes=item_list)\n",
    "train_model(vanilla_model, train_dataset, evaluation_dataset, num_epochs=10, MODEL_TYPE=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj3vLT1eXFnk"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkzG1i3AW1O7",
    "outputId": "ec7971c3-66ef-4a57-e710-248cb53dee8e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "add_detections(model, evaluation_dataset, fo_dataset, field_name=\"predictions\")\n",
    "\n",
    "results = fo.evaluate_detections(\n",
    "    test_view,\n",
    "    \"predictions\",\n",
    "    classes=item_list,\n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uYdXrhgYdJ_",
    "outputId": "2eb792e9-342f-4dc8-f6c0-e1503d8bf193",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results.mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcJBOM76aJPR",
    "outputId": "ac452527-7608-4e8d-f57e-18a0470acd30",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nddFfGSnXo7i"
   },
   "source": [
    "By default, objects are only matched with other objects of the same class. In order to get an interesting confusion matrix, we need to match interclass objects by setting `classwise=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_53aCMna2Vt",
    "outputId": "4db71f31-73e3-4036-f623-efd8e2ac85bf",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results_interclass = fo.evaluate_detections(\n",
    "    test_view, \n",
    "    \"predictions\", \n",
    "    classes=item_list,\n",
    "    compute_mAP=True, \n",
    "    classwise=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot = results.plot_pr_curves(classes=item_list)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Nbqf-NuAZ7Ps",
    "outputId": "571cd947-c94a-4b9a-ed93-2330fbddea7e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results_interclass.plot_confusion_matrix(classes=item_list, include_other=False, include_missing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElSV7tTbYKLr"
   },
   "source": [
    "The [detection evaluation](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html#detections) also added the attributes `eval_fp`, `eval_tp`, and `eval_fn` to every predicted detection indicating if it is a false positive, true positive, or false negative. \n",
    "Let's create a view to find the worst samples by sorting by `eval_fp` using the [FiftyOne App](https://voxel51.com/docs/fiftyone/user_guide/app.html) to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786,
     "resources": {
      "https://localhost:5151/polling?sessionId=de0b710e-15f8-4c57-ba46-ae7955f716b1": {
       "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
       "headers": [
        [
         "access-control-allow-headers",
         "x-requested-with"
        ],
        [
         "content-type",
         "text/html; charset=UTF-8"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "Pm4Z52rd8AC1",
    "outputId": "62d39076-7ef3-4fe3-95ae-500d0f8f8a3f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786,
     "resources": {
      "https://localhost:5151/polling?sessionId=ebbc318d-3578-4fb1-9ae7-68596117572b": {
       "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
       "headers": [
        [
         "access-control-allow-headers",
         "x-requested-with"
        ],
        [
         "content-type",
         "text/html; charset=UTF-8"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "njLG0l5K-ucV",
    "outputId": "bda6f02d-d8fe-49be-d212-31e0e70779e3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReXDVFgLZLtf"
   },
   "source": [
    "It would be best to get this [data reannotated to fix these mistakes](https://towardsdatascience.com/managing-annotation-mistakes-with-fiftyone-and-labelbox-fc6e87b51102), but in the meantime, we can easily remedy this by simply creating a new view that remaps the labels `car`, `truck`, and `bus` all to `vehicle` and then retraining the model with that. This is only possible because we are backing our data in FiftyOne and loading views into PyTorch as needed. Without FiftyOne, the PyTorch dataset class or the underlying data would need to be changed to remap these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# map labels to single vehicle class\n",
    "vehicle_list = ['car', 'bus', 'truck']\n",
    "vehicles_map = {c: \"vehicle\" for c in vehicle_list}\n",
    "\n",
    "train_map_view = train_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "test_map_view = test_view.map_labels(\"ground_truth\", vehicles_map)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "torch_map_dataset = FiftyOneTorchDataset(train_map_view, train_transforms)\n",
    "torch_map_dataset_test = FiftyOneTorchDataset(test_map_view, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynRCHQv8XB_v",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Only 2 classes (background and vehicle)\n",
    "MODEL_TYPE = 'Vanilla-FRCNN'\n",
    "vehicle_model = create_model(MODEL_TYPE, num_classes=(len(vehicles_map)+1))\n",
    "train_model(vehicle_model, torch_map_dataset, torch_map_dataset_test, num_epochs=2, MODEL_TYPE=MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-mrVOl4XFbp",
    "outputId": "6d8bec76-ebe8-4a36-959a-52bb1aab8498",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "add_detections(vehicle_model, torch_map_dataset_test, test_map_view, field_name=\"vehicle_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfd3xvhaXhl_",
    "outputId": "d9c4a2fe-538a-4979-c3f8-f5ede0c98aa1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vehicle_results = fo.evaluate_detections(\n",
    "    test_map_view, \n",
    "    \"vehicle_predictions\", \n",
    "    classes=[\"vehicle\"], \n",
    "    eval_key=\"vehicle_eval\", \n",
    "    compute_mAP=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFvddH3rk0NR",
    "outputId": "59572ba2-f9ad-4dd2-e9ac-90877190ff99",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vehicle_results.mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwbhq18sk1PL",
    "outputId": "d6985867-5049-4678-cc88-d5041a0079ed",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vehicle_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJMAkJbWZ_u1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Due to our ability to easily visualize and manage our dataset with FiftyOne, we were able to spot and take action on a dataset issue that would otherwise have gone unnoticed if we only concerned ourselves with dataset-wide evaluation metrics and fixed dataset representations. Through these efforts, we managed to increase the mAP of the model to 43%.\n",
    "\n",
    "Even though this example workflow may not work in all situations, this kind of class-merging strategy can be effective in cases where more fine-grained discrimination is not called for."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fiftyone_pytorch_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "torch-frcnn",
   "language": "python",
   "display_name": "torch-frcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32b6ec3046e64d04b4134553dc434fe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a4788a4fd6841788b20cfbf54a3d10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d836b94d13e459d82429606496e4d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1645bdfb02b42fba268f7000f183639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c063e7d90f6a4027b53d1b70c8c07742",
      "placeholder": "​",
      "style": "IPY_MODEL_a410071b34034a91aeda7ef1114969c2",
      "value": " 160M/160M [01:05&lt;00:00, 2.55MB/s]"
     }
    },
    "a410071b34034a91aeda7ef1114969c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acbb3df601244291b8b2fb9ea1137573": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8c6a316609d4ca5bfee139b93177ef5",
       "IPY_MODEL_a1645bdfb02b42fba268f7000f183639"
      ],
      "layout": "IPY_MODEL_32b6ec3046e64d04b4134553dc434fe0"
     }
    },
    "c063e7d90f6a4027b53d1b70c8c07742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8c6a316609d4ca5bfee139b93177ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d836b94d13e459d82429606496e4d4f",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a4788a4fd6841788b20cfbf54a3d10b",
      "value": 167502836
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}