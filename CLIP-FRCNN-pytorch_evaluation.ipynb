{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ2cvwpPWXBt",
    "outputId": "6444f42e-7465-4625-e4b1-3c207385fce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x2ccbc64e750>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "from dataset import FiftyOneTorchDataset\n",
    "from model import create_model\n",
    "from utils import add_detections, get_transforms\n",
    "\n",
    "import config\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# dataset_name = \"coco-2017-validation\"\n",
    "dataset_name = \"ImageNet_validation\"\n",
    "\n",
    "# The directory containing the dataset to import\n",
    "dataset_dir = \"C:/Data_drive/Data/Imagenet/ImageNet/imagenet_val_dataset/imagenet_val_dataset\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#fo.core.dataset.delete_non_persistent_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 50000/50000 [3.9m elapsed, 0s remaining, 183.4 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2cc96f2cbe0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=9c4772c5-cb51-4940-bb90-c240ba713162\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the datasets exist on this machine\n",
    "if fo.core.dataset.dataset_exists(dataset_name):\n",
    "\n",
    "    fo_dataset = fo.load_dataset(dataset_name) # if the dataset  exists, load it\n",
    "else:\n",
    "    if dataset_name == \"coco-2017-validation\":\n",
    "        fo_dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")\n",
    "    else: # if the dataset isnt coco, we will load it from the machine\n",
    "        fo_dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_type=fo.types.VOCDetectionDataset,\n",
    "        name=dataset_name,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if dataset_name == \"ImageNet_validation\":\n",
    "    with open('dataset_analysis/imagenet_dict_mapping.pkl', 'rb') as f:\n",
    "                imagenet_class_mapping = pickle.load(f)\n",
    "    fo_dataset = fo_dataset.map_labels(\"ground_truth\", imagenet_class_mapping)\n",
    "\n",
    "\n",
    "#needed to calculate image height and width\n",
    "fo_dataset.compute_metadata()\n",
    "#create the session to view the dataset\n",
    "session = fo.launch_app(fo_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# create the list of labels needed for evaluation, if evaluating on all labels, leave empty\n",
    "\n",
    "known_unknowns = ['lizard', 'turtle', 'pen', 'cowboy hat', 'tank']\n",
    "known_knowns = ['clock', 'vase', 'toaster', 'microwave', 'mouse', 'potted plant', 'sports ball', 'zebra', 'dog', 'bird', 'bench', 'parking meter', 'airplane', 'bicycle']\n",
    "\n",
    "\n",
    "\n",
    "dataset_class_labels = known_knowns + known_unknowns\n",
    "\n",
    "model_class_labels = known_knowns + known_unknowns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 742 known samples\n",
      "Evaluating on 271 unknown samples\n",
      "Evaluating on 1013 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2cc94c96100>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=db6e4b96-0e2a-492c-8afa-897f961f8e75\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the transformations needed for the images\n",
    "_, test_transforms = get_transforms()\n",
    "\n",
    "if len(dataset_class_labels) > 0:\n",
    "\n",
    "    item_view = fo_dataset.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(dataset_class_labels))\n",
    "\n",
    "    # find the class with the fewest examples\n",
    "    class_count = item_view.count_values(\"ground_truth.detections.label\")\n",
    "    smallest_class = min(class_count, key=class_count.get) # find the key of the smallest class\n",
    "\n",
    "    id = set() # create a set to contain the image ids\n",
    "\n",
    "    for dataset_class in item_view.distinct(\"ground_truth.detections.label\"): # loop through all of the class labels\n",
    "        class_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(dataset_class)) # create a view from which to sample the class\n",
    "        sample_ids = class_view.take(class_count[smallest_class], seed = 51) # take the number of classes based on the smallest class\n",
    "\n",
    "        for sample in sample_ids:\n",
    "            id.add(sample.id) # add the image ids to the set\n",
    "    item_view = item_view.select(id) # create a view based on these images\n",
    "\n",
    "    known_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(known_knowns))\n",
    "\n",
    "\n",
    "    unknown_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(known_unknowns))\n",
    "\n",
    "\n",
    "    # use our dataset and defined transformations\n",
    "    known_evaluation_dataset = FiftyOneTorchDataset(known_view, test_transforms,\n",
    "        classes=known_knowns)\n",
    "\n",
    "    # use our dataset and defined transformations\n",
    "    unknown_evaluation_dataset = FiftyOneTorchDataset(unknown_view, test_transforms,\n",
    "        classes=known_unknowns)\n",
    "\n",
    "    print(f'Evaluating on {len(known_evaluation_dataset)} known samples')\n",
    "    print(f'Evaluating on {len(unknown_evaluation_dataset)} unknown samples')\n",
    "else: # if we do not provide labels of interest\n",
    "    item_view = fo_dataset\n",
    "\n",
    "    #create an item list for use later\n",
    "    dataset_class_labels = fo_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "\n",
    "print(f'Evaluating on {len(item_view)} samples')\n",
    "\n",
    "\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "evaluation_dataset = FiftyOneTorchDataset(item_view, test_transforms,\n",
    "        classes=dataset_class_labels)\n",
    "\n",
    "session.view = item_view\n",
    "\n",
    "# add a blank line dropped during classification\n",
    "if model_class_labels[0] != 'background':\n",
    "     model_class_labels.insert(0,'background')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'clock', 'vase', 'toaster', 'microwave', 'computer mouse', 'potted plant', 'sports ball', 'zebra', 'dog', 'bird', 'bench', 'parking meter', 'airplane', 'bicycle', 'lizard', 'turtle', 'pen', 'cowboy hat', 'tank']\n"
     ]
    }
   ],
   "source": [
    "# for some items, CLIP may do better with different textual descriptions\n",
    "\n",
    "replacements = {\n",
    "    'mouse': 'computer mouse',\n",
    "}\n",
    "\n",
    "for k, v in replacements.items():\n",
    "    CLIP_list = [v if item == k else item for item in model_class_labels]\n",
    "\n",
    "print(CLIP_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check CLIP RPN performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint at epoch 30\n"
     ]
    },
    {
     "data": {
      "text/plain": "ZeroShotOD(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): FeatureExtractor(\n    (model): ModifiedResNet(\n      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n      (relu): ReLU(inplace=True)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (attnpool): Identity()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(2048, 9, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(2048, 36, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (classifier): CLIPRPNPredictor(\n    (image_embedder): AttentionPool2d(\n      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n    )\n  )\n  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=8)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the trained CLIP-FRCNN\n",
    "MODEL_TYPE = 'CLIP-RPN'\n",
    "WEIGHTS_NAME='CLIP-RPN_rpn_full_training epoch_30.pth'\n",
    "\n",
    "# tokenize item list for CLIP\n",
    "import clip\n",
    "_, preprocess = clip.load(\"RN50\", device=config.DEVICE)\n",
    "\n",
    "# create the model\n",
    "clip_frcnn_model = create_model(MODEL_TYPE, classes=CLIP_list)\n",
    "\n",
    "# load the pre-trained model\n",
    "checkpoint = torch.load(WEIGHTS_NAME)\n",
    "clip_frcnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "print(f'loaded checkpoint at epoch {epoch}')\n",
    "\n",
    "# set to evaluation mode\n",
    "clip_frcnn_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# a helper funciton to evaluate the model on certain datasets\n",
    "def evaluate_model(model , field_name, evaluation_dataset, fiftyone_Dataset, classes = dataset_class_labels, BCC=False, iou=0.5, eps = 35, labelmap=dataset_class_labels):\n",
    "    # model - the object detection model\n",
    "    # field_name - the name used to add the model to FiftyOne\n",
    "    # evaluation_dataset - the FiftyOne view on which to calculate the data\n",
    "    # fiftyone_Dataset - the full FiftyOne dataset\n",
    "    # Classes - the class list on which to evaluate the model\n",
    "    # BCC - use Bounding box Clustering and Consolidation\n",
    "    # iou - the iou threshold for nms\n",
    "    # eps - the eps_neighborhood parameter from DBSCAN\n",
    "    # labelmap - the mapping of model class number to dataset class number\n",
    "\n",
    "    add_detections(model, evaluation_dataset, fiftyone_Dataset, field_name=field_name, PRED_CLUSTERING=BCC, labelmap = labelmap, eps = eps)\n",
    "\n",
    "    evaluation = fo.evaluate_detections(\n",
    "        item_view,\n",
    "        field_name,\n",
    "        classes=classes,\n",
    "        eval_key=field_name,\n",
    "        compute_mAP=True,\n",
    "        iou=iou,\n",
    "    )\n",
    "\n",
    "    return evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [4.4m elapsed, 0s remaining, 10.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [22.9s elapsed, 0s remaining, 48.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [37.4s elapsed, 0s remaining, 30.4 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [24.6s elapsed, 0s remaining, 11.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [28.4s elapsed, 0s remaining, 37.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [46.5s elapsed, 0s remaining, 27.4 samples/s]      \n",
      "Known mAP (no BCC): 0.007001068181136371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.02      0.52      0.05        63\n",
      "         vase       0.04      0.54      0.07        70\n",
      "      toaster       0.03      0.36      0.05        64\n",
      "    microwave       0.02      0.61      0.04        57\n",
      "        mouse       0.01      0.62      0.02        65\n",
      " potted plant       0.03      0.78      0.06       165\n",
      "  sports ball       0.02      0.67      0.03        94\n",
      "        zebra       0.03      0.99      0.06        88\n",
      "          dog       0.02      0.99      0.04        74\n",
      "         bird       0.02      0.93      0.04        69\n",
      "        bench       0.01      0.84      0.03        80\n",
      "parking meter       0.02      0.95      0.04        76\n",
      "     airplane       0.01      0.99      0.02        69\n",
      "      bicycle       0.02      0.95      0.04        79\n",
      "\n",
      "    micro avg       0.02      0.78      0.04      1113\n",
      "    macro avg       0.02      0.77      0.04      1113\n",
      " weighted avg       0.02      0.78      0.04      1113\n",
      "\n",
      "Unknown mAP (no BCC): 0.006235254286983825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.07      0.62      0.12        63\n",
      "      turtle       0.06      0.55      0.11        75\n",
      "         pen       0.04      0.95      0.07       102\n",
      "  cowboy hat       0.06      0.76      0.10       108\n",
      "        tank       0.02      0.99      0.03        69\n",
      "\n",
      "   micro avg       0.04      0.78      0.07       417\n",
      "   macro avg       0.05      0.77      0.09       417\n",
      "weighted avg       0.05      0.78      0.09       417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance without clustering\n",
    "\n",
    "iou = .1\n",
    "known_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = known_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_knowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "unknown_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_unknowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "\n",
    "print(f'Known mAP (no BCC): {known_no_cluster_evaluation.mAP()}')\n",
    "known_no_cluster_evaluation.print_report()\n",
    "print(f'Unknown mAP (no BCC): {unknown_no_cluster_evaluation.mAP()}')\n",
    "unknown_no_cluster_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [1.6m elapsed, 0s remaining, 9.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [27.6s elapsed, 0s remaining, 42.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [46.0s elapsed, 0s remaining, 27.7 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [29.7s elapsed, 0s remaining, 9.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [27.9s elapsed, 0s remaining, 42.9 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [45.8s elapsed, 0s remaining, 28.0 samples/s]      \n",
      "Known mAP (no BCC): 0.007001068181136371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.02      0.43      0.04        63\n",
      "         vase       0.03      0.37      0.05        70\n",
      "      toaster       0.01      0.11      0.02        64\n",
      "    microwave       0.01      0.30      0.02        57\n",
      "        mouse       0.00      0.25      0.01        65\n",
      " potted plant       0.02      0.37      0.03       165\n",
      "  sports ball       0.01      0.36      0.02        94\n",
      "        zebra       0.03      0.90      0.05        88\n",
      "          dog       0.02      0.89      0.04        74\n",
      "         bird       0.02      0.78      0.03        69\n",
      "        bench       0.01      0.65      0.02        80\n",
      "parking meter       0.01      0.62      0.03        76\n",
      "     airplane       0.01      0.96      0.02        69\n",
      "      bicycle       0.02      0.81      0.03        79\n",
      "\n",
      "    micro avg       0.01      0.55      0.03      1113\n",
      "    macro avg       0.01      0.56      0.03      1113\n",
      " weighted avg       0.02      0.55      0.03      1113\n",
      "\n",
      "Unknown mAP (no BCC): 0.006235254286983825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.04      0.38      0.07        63\n",
      "      turtle       0.04      0.36      0.07        75\n",
      "         pen       0.02      0.56      0.04       102\n",
      "  cowboy hat       0.02      0.28      0.04       108\n",
      "        tank       0.01      0.83      0.03        69\n",
      "\n",
      "   micro avg       0.02      0.47      0.04       417\n",
      "   macro avg       0.03      0.48      0.05       417\n",
      "weighted avg       0.03      0.47      0.05       417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance without clustering\n",
    "\n",
    "iou = .5\n",
    "known_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = known_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_knowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "unknown_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_unknowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "\n",
    "print(f'Known mAP (no BCC): {known_no_cluster_evaluation.mAP()}')\n",
    "known_no_cluster_evaluation.print_report()\n",
    "print(f'Unknown mAP (no BCC): {unknown_no_cluster_evaluation.mAP()}')\n",
    "unknown_no_cluster_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find performance with clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.2m elapsed, 0s remaining, 9.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.5s elapsed, 0s remaining, 186.4 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.7s elapsed, 0s remaining, 186.8 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.32      0.51      0.39        63\n",
      "         vase       0.38      0.50      0.43        70\n",
      "      toaster       0.14      0.25      0.18        64\n",
      "    microwave       0.23      0.51      0.32        57\n",
      "        mouse       0.04      0.35      0.07        65\n",
      " potted plant       0.21      0.41      0.28       165\n",
      "  sports ball       0.14      0.45      0.21        94\n",
      "        zebra       0.28      0.76      0.41        88\n",
      "          dog       0.15      0.86      0.25        74\n",
      "         bird       0.18      0.88      0.31        69\n",
      "        bench       0.15      0.60      0.23        80\n",
      "parking meter       0.19      0.71      0.30        76\n",
      "     airplane       0.22      0.93      0.36        69\n",
      "      bicycle       0.32      0.78      0.45        79\n",
      "       lizard       0.41      0.56      0.47        63\n",
      "       turtle       0.31      0.52      0.39        75\n",
      "          pen       0.16      0.71      0.26       102\n",
      "   cowboy hat       0.19      0.46      0.27       108\n",
      "         tank       0.10      0.87      0.18        69\n",
      "\n",
      "    micro avg       0.16      0.60      0.25      1530\n",
      "    macro avg       0.21      0.58      0.29      1530\n",
      " weighted avg       0.21      0.60      0.30      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps = 35\n",
    "iou = .1\n",
    "\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.3m elapsed, 0s remaining, 9.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.5s elapsed, 0s remaining, 187.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.6s elapsed, 0s remaining, 187.2 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.15      0.24      0.18        63\n",
      "         vase       0.18      0.23      0.20        70\n",
      "      toaster       0.03      0.06      0.04        64\n",
      "    microwave       0.06      0.14      0.09        57\n",
      "        mouse       0.01      0.11      0.02        65\n",
      " potted plant       0.06      0.12      0.08       165\n",
      "  sports ball       0.05      0.16      0.08        94\n",
      "        zebra       0.11      0.31      0.17        88\n",
      "          dog       0.11      0.64      0.19        74\n",
      "         bird       0.11      0.52      0.18        69\n",
      "        bench       0.05      0.21      0.08        80\n",
      "parking meter       0.07      0.25      0.11        76\n",
      "     airplane       0.06      0.23      0.09        69\n",
      "      bicycle       0.12      0.30      0.18        79\n",
      "       lizard       0.09      0.13      0.11        63\n",
      "       turtle       0.16      0.27      0.20        75\n",
      "          pen       0.05      0.22      0.08       102\n",
      "   cowboy hat       0.05      0.11      0.06       108\n",
      "         tank       0.06      0.49      0.10        69\n",
      "\n",
      "    micro avg       0.06      0.24      0.10      1530\n",
      "    macro avg       0.08      0.24      0.11      1530\n",
      " weighted avg       0.08      0.24      0.11      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps = 35\n",
    "iou=0.5\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 394/394 [1.0m elapsed, 0s remaining, 8.0 samples/s]       \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [4.2s elapsed, 0s remaining, 240.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [3.8s elapsed, 0s remaining, 295.3 samples/s]      \n",
      "mAP = 0.00485038880779554\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.10      0.06      0.08        63\n",
      "         vase       0.10      0.07      0.08        70\n",
      "      toaster       0.00      0.00      0.00        64\n",
      "    microwave       0.04      0.05      0.05        57\n",
      "        mouse       0.02      0.08      0.03        65\n",
      " potted plant       0.04      0.06      0.05       165\n",
      "  sports ball       0.04      0.06      0.05        94\n",
      "        zebra       0.07      0.08      0.07        88\n",
      "          dog       0.05      0.11      0.07        74\n",
      "         bird       0.04      0.09      0.06        69\n",
      "        bench       0.02      0.05      0.03        80\n",
      "parking meter       0.04      0.07      0.05        76\n",
      "     airplane       0.05      0.07      0.06        69\n",
      "      bicycle       0.03      0.04      0.04        79\n",
      "       lizard       0.00      0.00      0.00        63\n",
      "       turtle       0.00      0.00      0.00        75\n",
      "          pen       0.02      0.03      0.02       102\n",
      "   cowboy hat       0.01      0.01      0.01       108\n",
      "         tank       0.04      0.13      0.06        69\n",
      "\n",
      "    micro avg       0.03      0.05      0.04      1530\n",
      "    macro avg       0.03      0.05      0.04      1530\n",
      " weighted avg       0.04      0.05      0.04      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on small objects only\n",
    "# Bboxes are in [top-left-x, top-left-y, width, height] format\n",
    "bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "small_view = item_view.filter_labels(\"ground_truth\", bbox_area < 0.2)\n",
    "# use our dataset and defined transformations\n",
    "small_object_dataset = FiftyOneTorchDataset(small_view, test_transforms,\n",
    "        classes=dataset_class_labels)\n",
    "iou=0.5\n",
    "eps = 35\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = small_object_dataset,\n",
    "                                field_name = f\"clip_RPN_small_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 699/699 [1.8m elapsed, 0s remaining, 7.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [7.3s elapsed, 0s remaining, 137.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [7.7s elapsed, 0s remaining, 135.8 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.15      0.24      0.18        63\n",
      "         vase       0.18      0.23      0.20        70\n",
      "      toaster       0.03      0.06      0.04        64\n",
      "    microwave       0.06      0.14      0.09        57\n",
      "        mouse       0.01      0.11      0.02        65\n",
      " potted plant       0.06      0.12      0.08       165\n",
      "  sports ball       0.05      0.16      0.08        94\n",
      "        zebra       0.11      0.31      0.17        88\n",
      "          dog       0.11      0.64      0.19        74\n",
      "         bird       0.11      0.52      0.18        69\n",
      "        bench       0.05      0.21      0.08        80\n",
      "parking meter       0.07      0.25      0.11        76\n",
      "     airplane       0.06      0.23      0.09        69\n",
      "      bicycle       0.12      0.30      0.18        79\n",
      "       lizard       0.09      0.13      0.11        63\n",
      "       turtle       0.16      0.27      0.20        75\n",
      "          pen       0.05      0.22      0.08       102\n",
      "   cowboy hat       0.05      0.11      0.06       108\n",
      "         tank       0.06      0.49      0.10        69\n",
      "\n",
      "    micro avg       0.06      0.24      0.10      1530\n",
      "    macro avg       0.08      0.24      0.11      1530\n",
      " weighted avg       0.08      0.24      0.11      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on large objects only\n",
    "# Bboxes are in [top-left-x, top-left-y, width, height] format\n",
    "bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "large_view = item_view.filter_labels(\"ground_truth\", bbox_area > 0.2)\n",
    "# use our dataset and defined transformations\n",
    "large_object_dataset = FiftyOneTorchDataset(large_view, test_transforms,\n",
    "        classes=dataset_class_labels)\n",
    "\n",
    "iou=0.5\n",
    "eps = 35\n",
    "\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = large_object_dataset,\n",
    "                                field_name = f\"clip_RPN_large_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sensitivity Study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 5\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.3m elapsed, 0s remaining, 9.3 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.0s elapsed, 0s remaining, 202.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.1s elapsed, 0s remaining, 206.7 samples/s]      \n",
      "mAP = 0.013848216871582131\n",
      "epsilon = 10\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 9.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.7s elapsed, 0s remaining, 175.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.6s elapsed, 0s remaining, 191.3 samples/s]      \n",
      "mAP = 0.014490400290133697\n",
      "epsilon = 15\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 8.9 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.8s elapsed, 0s remaining, 183.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.8s elapsed, 0s remaining, 183.2 samples/s]      \n",
      "mAP = 0.014768878102337428\n",
      "epsilon = 20\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 8.9 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.9s elapsed, 0s remaining, 168.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.1s elapsed, 0s remaining, 177.0 samples/s]      \n",
      "mAP = 0.015959547900688507\n",
      "epsilon = 25\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.5m elapsed, 0s remaining, 8.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.9s elapsed, 0s remaining, 175.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 174.1 samples/s]      \n",
      "mAP = 0.015211357096605739\n",
      "epsilon = 30\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.5m elapsed, 0s remaining, 8.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 164.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.4s elapsed, 0s remaining, 166.7 samples/s]      \n",
      "mAP = 0.01556719808789616\n",
      "epsilon = 35\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 165.7 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.4s elapsed, 0s remaining, 164.7 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "epsilon = 40\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.3s elapsed, 0s remaining, 156.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.7s elapsed, 0s remaining, 162.2 samples/s]      \n",
      "mAP = 0.0158219613251373\n",
      "epsilon = 45\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.3 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.5s elapsed, 0s remaining, 157.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.6s elapsed, 0s remaining, 156.5 samples/s]      \n",
      "mAP = 0.014895660763696708\n",
      "best_mAP = 0.016176742240240776\n",
      "best_eps = 35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.15      0.24      0.18        63\n",
      "         vase       0.18      0.23      0.20        70\n",
      "      toaster       0.03      0.06      0.04        64\n",
      "    microwave       0.06      0.14      0.09        57\n",
      "        mouse       0.01      0.11      0.02        65\n",
      " potted plant       0.06      0.12      0.08       165\n",
      "  sports ball       0.05      0.16      0.08        94\n",
      "        zebra       0.11      0.31      0.17        88\n",
      "          dog       0.11      0.64      0.19        74\n",
      "         bird       0.11      0.52      0.18        69\n",
      "        bench       0.05      0.21      0.08        80\n",
      "parking meter       0.07      0.25      0.11        76\n",
      "     airplane       0.06      0.23      0.09        69\n",
      "      bicycle       0.12      0.30      0.18        79\n",
      "       lizard       0.09      0.13      0.11        63\n",
      "       turtle       0.16      0.27      0.20        75\n",
      "          pen       0.05      0.22      0.08       102\n",
      "   cowboy hat       0.05      0.11      0.06       108\n",
      "         tank       0.06      0.49      0.10        69\n",
      "\n",
      "    micro avg       0.06      0.24      0.10      1530\n",
      "    macro avg       0.08      0.24      0.11      1530\n",
      " weighted avg       0.08      0.24      0.11      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance with clustering\n",
    "\n",
    "results = []\n",
    "best_map = 0\n",
    "\n",
    "for eps in range(5, 50, 5):\n",
    "    print(f'epsilon = {eps}')\n",
    "\n",
    "    evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=0.5,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "    map = evaluation.mAP()\n",
    "    print(f'mAP = {map}')\n",
    "\n",
    "    results.append([eps, map])\n",
    "    if map > best_map:\n",
    "        best_eps = eps\n",
    "        best_map = map\n",
    "        best_eval = evaluation\n",
    "\n",
    "print(f'best_mAP = {best_map}')\n",
    "print(f'best_eps = {best_eps}')\n",
    "best_eval.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing the required module\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "res = pd.DataFrame(results)\n",
    "# plotting the points\n",
    "plt.plot(res[0], res[1])\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('DBSCAN eps')\n",
    "# naming the y axis\n",
    "plt.ylabel('mean Average Precision')\n",
    "\n",
    "# giving a title to my graph\n",
    "plt.title('Box Clustering and Consolidation Sensitivity')\n",
    "\n",
    "# function to show the plot\n",
    "\n",
    "plt.savefig('Box Clustering and Consolidation Sensitivity.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test against Faster-RCNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# create the labelmap for the coco dataset\n",
    "coco_labels = open(\"dataset_analysis/coco_labels.txt\", \"r\")\n",
    "coco_list = coco_labels.read().splitlines() # read each line in as a value in a list\n",
    "coco_list.insert(0,'background') # add the background class\n",
    "coco_id = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
    "          35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "          64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91] # annoyingly, COCO has 90 class ids but only 80 labels\n",
    "coco = dict(zip(coco_id, coco_list)) # convert it to a dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate metrics from the FRCNN using iou 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [1.9m elapsed, 0s remaining, 7.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [10.3s elapsed, 0s remaining, 92.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [13.8s elapsed, 0s remaining, 81.9 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [41.3s elapsed, 0s remaining, 6.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.3s elapsed, 0s remaining, 159.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.5s elapsed, 0s remaining, 160.6 samples/s]      \n",
      "mAP: 0.43711472188549855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.24      0.84      0.37        63\n",
      "         vase       0.17      0.90      0.28        70\n",
      "      toaster       0.52      0.53      0.53        64\n",
      "    microwave       0.31      0.79      0.44        57\n",
      "        mouse       0.36      0.80      0.50        65\n",
      " potted plant       0.16      0.81      0.26       165\n",
      "  sports ball       0.50      0.68      0.58        94\n",
      "        zebra       0.32      1.00      0.49        88\n",
      "          dog       0.38      0.99      0.55        74\n",
      "         bird       0.23      0.97      0.37        69\n",
      "        bench       0.15      0.96      0.26        80\n",
      "parking meter       0.37      0.93      0.53        76\n",
      "     airplane       0.32      1.00      0.48        69\n",
      "      bicycle       0.14      0.95      0.25        79\n",
      "\n",
      "    micro avg       0.23      0.87      0.37      1113\n",
      "    macro avg       0.30      0.87      0.42      1113\n",
      " weighted avg       0.29      0.87      0.41      1113\n",
      "\n",
      "mAP: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.00      0.00      0.00      63.0\n",
      "      turtle       0.00      0.00      0.00      75.0\n",
      "         pen       0.00      0.00      0.00     102.0\n",
      "  cowboy hat       0.00      0.00      0.00     108.0\n",
      "        tank       0.00      0.00      0.00      69.0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     417.0\n",
      "   macro avg       0.00      0.00      0.00     417.0\n",
      "weighted avg       0.00      0.00      0.00     417.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "iou=.1\n",
    "pre_trained_known_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = known_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_known_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_knowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "pre_trained_unknown_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_unknown_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_unknowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "\n",
    "print(f'mAP: {pre_trained_known_evaluation.mAP()}')\n",
    "pre_trained_known_evaluation.print_report()\n",
    "\n",
    "print(f'mAP: {pre_trained_unknown_evaluation.mAP()}')\n",
    "pre_trained_unknown_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [2.0m elapsed, 0s remaining, 7.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [9.9s elapsed, 0s remaining, 100.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [12.3s elapsed, 0s remaining, 83.4 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [39.7s elapsed, 0s remaining, 7.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.1s elapsed, 0s remaining, 168.0 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 149.4 samples/s]      \n",
      "mAP: 0.43711472188549855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.20      0.68      0.30        63\n",
      "         vase       0.17      0.90      0.28        70\n",
      "      toaster       0.51      0.52      0.51        64\n",
      "    microwave       0.29      0.75      0.42        57\n",
      "        mouse       0.34      0.75      0.47        65\n",
      " potted plant       0.09      0.48      0.16       165\n",
      "  sports ball       0.49      0.66      0.56        94\n",
      "        zebra       0.31      0.97      0.47        88\n",
      "          dog       0.37      0.97      0.54        74\n",
      "         bird       0.22      0.96      0.36        69\n",
      "        bench       0.14      0.91      0.25        80\n",
      "parking meter       0.31      0.79      0.44        76\n",
      "     airplane       0.31      0.96      0.46        69\n",
      "      bicycle       0.14      0.91      0.24        79\n",
      "\n",
      "    micro avg       0.21      0.78      0.33      1113\n",
      "    macro avg       0.28      0.80      0.39      1113\n",
      " weighted avg       0.26      0.78      0.37      1113\n",
      "\n",
      "mAP: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.00      0.00      0.00      63.0\n",
      "      turtle       0.00      0.00      0.00      75.0\n",
      "         pen       0.00      0.00      0.00     102.0\n",
      "  cowboy hat       0.00      0.00      0.00     108.0\n",
      "        tank       0.00      0.00      0.00      69.0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     417.0\n",
      "   macro avg       0.00      0.00      0.00     417.0\n",
      "weighted avg       0.00      0.00      0.00     417.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iou=.5\n",
    "pre_trained_known_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = known_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_known_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_knowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "pre_trained_unknown_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_unknown_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_unknowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "\n",
    "print(f'mAP: {pre_trained_known_evaluation.mAP()}')\n",
    "pre_trained_known_evaluation.print_report()\n",
    "\n",
    "print(f'mAP: {pre_trained_unknown_evaluation.mAP()}')\n",
    "pre_trained_unknown_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FigureWidget({\n    'data': [{'customdata': array([0.99967682, 0.99964222, 0.99950137, 0.99922658, 0.99922658, …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bc0d326e96440ed92ef0384ae5ec496"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = pre_trained_known_evaluation.plot_pr_curves(classes=known_knowns)\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FigureWidget({\n    'data': [{'customdata': array([0.99996078, 0.59719999, 0.49619357, 0.39688331, 0.39688331, …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66067330a1244d0eb16b8295368ed06f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot2 = BCC_evaluation.plot_pr_curves(classes=known_unknowns)\n",
    "plot2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2cc95d7b550>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=1503e7a8-aba3-4074-b0b7-19fb2021d321\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "session.view = item_view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fiftyone_pytorch_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "torch-frcnn",
   "language": "python",
   "display_name": "torch-frcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32b6ec3046e64d04b4134553dc434fe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a4788a4fd6841788b20cfbf54a3d10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d836b94d13e459d82429606496e4d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1645bdfb02b42fba268f7000f183639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c063e7d90f6a4027b53d1b70c8c07742",
      "placeholder": "​",
      "style": "IPY_MODEL_a410071b34034a91aeda7ef1114969c2",
      "value": " 160M/160M [01:05&lt;00:00, 2.55MB/s]"
     }
    },
    "a410071b34034a91aeda7ef1114969c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acbb3df601244291b8b2fb9ea1137573": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8c6a316609d4ca5bfee139b93177ef5",
       "IPY_MODEL_a1645bdfb02b42fba268f7000f183639"
      ],
      "layout": "IPY_MODEL_32b6ec3046e64d04b4134553dc434fe0"
     }
    },
    "c063e7d90f6a4027b53d1b70c8c07742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8c6a316609d4ca5bfee139b93177ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d836b94d13e459d82429606496e4d4f",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a4788a4fd6841788b20cfbf54a3d10b",
      "value": 167502836
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}