{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ2cvwpPWXBt",
    "outputId": "6444f42e-7465-4625-e4b1-3c207385fce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x2ccbc64e750>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "from dataset import FiftyOneTorchDataset\n",
    "from model import create_model\n",
    "from utils import add_detections, get_transforms\n",
    "\n",
    "import config\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# dataset_name = \"coco-2017-validation\"\n",
    "dataset_name = \"ImageNet_validation\"\n",
    "\n",
    "# The directory containing the dataset to import\n",
    "dataset_dir = \"C:/Data_drive/Data/Imagenet/ImageNet/imagenet_val_dataset/imagenet_val_dataset\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#fo.core.dataset.delete_non_persistent_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 50000/50000 [3.9m elapsed, 0s remaining, 183.4 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2cc96f2cbe0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=9c4772c5-cb51-4940-bb90-c240ba713162\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the datasets exist on this machine\n",
    "if fo.core.dataset.dataset_exists(dataset_name):\n",
    "\n",
    "    fo_dataset = fo.load_dataset(dataset_name) # if the dataset  exists, load it\n",
    "else:\n",
    "    if dataset_name == \"coco-2017-validation\":\n",
    "        fo_dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")\n",
    "    else: # if the dataset isnt coco, we will load it from the machine\n",
    "        fo_dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_type=fo.types.VOCDetectionDataset,\n",
    "        name=dataset_name,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if dataset_name == \"ImageNet_validation\":\n",
    "    with open('dataset_analysis/imagenet_dict_mapping.pkl', 'rb') as f:\n",
    "                imagenet_class_mapping = pickle.load(f)\n",
    "    fo_dataset = fo_dataset.map_labels(\"ground_truth\", imagenet_class_mapping)\n",
    "\n",
    "\n",
    "#needed to calculate image height and width\n",
    "fo_dataset.compute_metadata()\n",
    "#create the session to view the dataset\n",
    "session = fo.launch_app(fo_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# create the list of labels needed for evaluation, if evaluating on all labels, leave empty\n",
    "\n",
    "known_unknowns = ['lizard', 'turtle', 'pen', 'cowboy hat', 'tank']\n",
    "known_knowns = ['clock', 'vase', 'toaster', 'microwave', 'mouse', 'potted plant', 'sports ball', 'zebra', 'dog', 'bird', 'bench', 'parking meter', 'airplane', 'bicycle']\n",
    "\n",
    "\n",
    "\n",
    "dataset_class_labels = known_knowns + known_unknowns\n",
    "\n",
    "model_class_labels = known_knowns + known_unknowns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 742 known samples\n",
      "Evaluating on 271 unknown samples\n",
      "Evaluating on 1013 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2cc94c96100>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=true&handleId=db6e4b96-0e2a-492c-8afa-897f961f8e75\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the transformations needed for the images\n",
    "_, test_transforms = get_transforms()\n",
    "\n",
    "if len(dataset_class_labels) > 0:\n",
    "\n",
    "    item_view = fo_dataset.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(dataset_class_labels))\n",
    "\n",
    "    # find the class with the fewest examples\n",
    "    class_count = item_view.count_values(\"ground_truth.detections.label\")\n",
    "    smallest_class = min(class_count, key=class_count.get) # find the key of the smallest class\n",
    "\n",
    "    id = set() # create a set to contain the image ids\n",
    "\n",
    "    for dataset_class in item_view.distinct(\"ground_truth.detections.label\"): # loop through all of the class labels\n",
    "        class_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(dataset_class)) # create a view from which to sample the class\n",
    "        sample_ids = class_view.take(class_count[smallest_class], seed = 51) # take the number of classes based on the smallest class\n",
    "\n",
    "        for sample in sample_ids:\n",
    "            id.add(sample.id) # add the image ids to the set\n",
    "    item_view = item_view.select(id) # create a view based on these images\n",
    "\n",
    "    known_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(known_knowns))\n",
    "\n",
    "\n",
    "    unknown_view = item_view.filter_labels(\"ground_truth\",\n",
    "            F(\"label\").is_in(known_unknowns))\n",
    "\n",
    "\n",
    "    # use our dataset and defined transformations\n",
    "    known_evaluation_dataset = FiftyOneTorchDataset(known_view, test_transforms,\n",
    "        classes=known_knowns)\n",
    "\n",
    "    # use our dataset and defined transformations\n",
    "    unknown_evaluation_dataset = FiftyOneTorchDataset(unknown_view, test_transforms,\n",
    "        classes=known_unknowns)\n",
    "\n",
    "    print(f'Evaluating on {len(known_evaluation_dataset)} known samples')\n",
    "    print(f'Evaluating on {len(unknown_evaluation_dataset)} unknown samples')\n",
    "else: # if we do not provide labels of interest\n",
    "    item_view = fo_dataset\n",
    "\n",
    "    #create an item list for use later\n",
    "    dataset_class_labels = fo_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "\n",
    "print(f'Evaluating on {len(item_view)} samples')\n",
    "\n",
    "\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "evaluation_dataset = FiftyOneTorchDataset(item_view, test_transforms,\n",
    "        classes=dataset_class_labels)\n",
    "\n",
    "session.view = item_view\n",
    "\n",
    "# add a blank line dropped during classification\n",
    "if model_class_labels[0] != 'background':\n",
    "     model_class_labels.insert(0,'background')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'clock', 'vase', 'toaster', 'microwave', 'computer mouse', 'potted plant', 'sports ball', 'zebra', 'dog', 'bird', 'bench', 'parking meter', 'airplane', 'bicycle', 'lizard', 'turtle', 'pen', 'cowboy hat', 'tank']\n"
     ]
    }
   ],
   "source": [
    "# for some items, CLIP may do better with different textual descriptions\n",
    "\n",
    "replacements = {\n",
    "    'mouse': 'computer mouse',\n",
    "}\n",
    "\n",
    "for k, v in replacements.items():\n",
    "    CLIP_list = [v if item == k else item for item in model_class_labels]\n",
    "\n",
    "print(CLIP_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check CLIP RPN performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint at epoch 30\n"
     ]
    },
    {
     "data": {
      "text/plain": "ZeroShotOD(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): FeatureExtractor(\n    (model): ModifiedResNet(\n      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n      (relu): ReLU(inplace=True)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (avgpool): Identity()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (attnpool): Identity()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(2048, 9, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(2048, 36, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (classifier): CLIPRPNPredictor(\n    (image_embedder): AttentionPool2d(\n      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n    )\n  )\n  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=8)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the trained CLIP-FRCNN\n",
    "MODEL_TYPE = 'CLIP-RPN'\n",
    "WEIGHTS_NAME='CLIP-RPN_rpn_full_training epoch_30.pth'\n",
    "\n",
    "# tokenize item list for CLIP\n",
    "import clip\n",
    "_, preprocess = clip.load(\"RN50\", device=config.DEVICE)\n",
    "\n",
    "# create the model\n",
    "clip_frcnn_model = create_model(MODEL_TYPE, classes=CLIP_list)\n",
    "\n",
    "# load the pre-trained model\n",
    "checkpoint = torch.load(WEIGHTS_NAME)\n",
    "clip_frcnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "print(f'loaded checkpoint at epoch {epoch}')\n",
    "\n",
    "# set to evaluation mode\n",
    "clip_frcnn_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# a helper funciton to evaluate the model on certain datasets\n",
    "def evaluate_model(model , field_name, evaluation_dataset, fiftyone_Dataset, classes = dataset_class_labels, BCC=False, iou=0.5, eps = 35, labelmap=dataset_class_labels):\n",
    "    # model - the object detection model\n",
    "    # field_name - the name used to add the model to FiftyOne\n",
    "    # evaluation_dataset - the FiftyOne view on which to calculate the data\n",
    "    # fiftyone_Dataset - the full FiftyOne dataset\n",
    "    # Classes - the class list on which to evaluate the model\n",
    "    # BCC - use Bounding box Clustering and Consolidation\n",
    "    # iou - the iou threshold for nms\n",
    "    # eps - the eps_neighborhood parameter from DBSCAN\n",
    "    # labelmap - the mapping of model class number to dataset class number\n",
    "\n",
    "    add_detections(model, evaluation_dataset, fiftyone_Dataset, field_name=field_name, PRED_CLUSTERING=BCC, labelmap = labelmap, eps = eps)\n",
    "\n",
    "    evaluation = fo.evaluate_detections(\n",
    "        item_view,\n",
    "        field_name,\n",
    "        classes=classes,\n",
    "        eval_key=field_name,\n",
    "        compute_mAP=True,\n",
    "        iou=iou,\n",
    "    )\n",
    "\n",
    "    return evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [4.4m elapsed, 0s remaining, 10.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [22.9s elapsed, 0s remaining, 48.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [37.4s elapsed, 0s remaining, 30.4 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [24.6s elapsed, 0s remaining, 11.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [28.4s elapsed, 0s remaining, 37.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [46.5s elapsed, 0s remaining, 27.4 samples/s]      \n",
      "Known mAP (no BCC): 0.007001068181136371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.02      0.52      0.05        63\n",
      "         vase       0.04      0.54      0.07        70\n",
      "      toaster       0.03      0.36      0.05        64\n",
      "    microwave       0.02      0.61      0.04        57\n",
      "        mouse       0.01      0.62      0.02        65\n",
      " potted plant       0.03      0.78      0.06       165\n",
      "  sports ball       0.02      0.67      0.03        94\n",
      "        zebra       0.03      0.99      0.06        88\n",
      "          dog       0.02      0.99      0.04        74\n",
      "         bird       0.02      0.93      0.04        69\n",
      "        bench       0.01      0.84      0.03        80\n",
      "parking meter       0.02      0.95      0.04        76\n",
      "     airplane       0.01      0.99      0.02        69\n",
      "      bicycle       0.02      0.95      0.04        79\n",
      "\n",
      "    micro avg       0.02      0.78      0.04      1113\n",
      "    macro avg       0.02      0.77      0.04      1113\n",
      " weighted avg       0.02      0.78      0.04      1113\n",
      "\n",
      "Unknown mAP (no BCC): 0.006235254286983825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.07      0.62      0.12        63\n",
      "      turtle       0.06      0.55      0.11        75\n",
      "         pen       0.04      0.95      0.07       102\n",
      "  cowboy hat       0.06      0.76      0.10       108\n",
      "        tank       0.02      0.99      0.03        69\n",
      "\n",
      "   micro avg       0.04      0.78      0.07       417\n",
      "   macro avg       0.05      0.77      0.09       417\n",
      "weighted avg       0.05      0.78      0.09       417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance without clustering\n",
    "\n",
    "iou = .1\n",
    "known_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = known_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_knowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "unknown_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_unknowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "\n",
    "print(f'Known mAP (no BCC): {known_no_cluster_evaluation.mAP()}')\n",
    "known_no_cluster_evaluation.print_report()\n",
    "print(f'Unknown mAP (no BCC): {unknown_no_cluster_evaluation.mAP()}')\n",
    "unknown_no_cluster_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [1.6m elapsed, 0s remaining, 9.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [27.6s elapsed, 0s remaining, 42.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [46.0s elapsed, 0s remaining, 27.7 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [29.7s elapsed, 0s remaining, 9.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [27.9s elapsed, 0s remaining, 42.9 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [45.8s elapsed, 0s remaining, 28.0 samples/s]      \n",
      "Known mAP (no BCC): 0.007001068181136371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.02      0.43      0.04        63\n",
      "         vase       0.03      0.37      0.05        70\n",
      "      toaster       0.01      0.11      0.02        64\n",
      "    microwave       0.01      0.30      0.02        57\n",
      "        mouse       0.00      0.25      0.01        65\n",
      " potted plant       0.02      0.37      0.03       165\n",
      "  sports ball       0.01      0.36      0.02        94\n",
      "        zebra       0.03      0.90      0.05        88\n",
      "          dog       0.02      0.89      0.04        74\n",
      "         bird       0.02      0.78      0.03        69\n",
      "        bench       0.01      0.65      0.02        80\n",
      "parking meter       0.01      0.62      0.03        76\n",
      "     airplane       0.01      0.96      0.02        69\n",
      "      bicycle       0.02      0.81      0.03        79\n",
      "\n",
      "    micro avg       0.01      0.55      0.03      1113\n",
      "    macro avg       0.01      0.56      0.03      1113\n",
      " weighted avg       0.02      0.55      0.03      1113\n",
      "\n",
      "Unknown mAP (no BCC): 0.006235254286983825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.04      0.38      0.07        63\n",
      "      turtle       0.04      0.36      0.07        75\n",
      "         pen       0.02      0.56      0.04       102\n",
      "  cowboy hat       0.02      0.28      0.04       108\n",
      "        tank       0.01      0.83      0.03        69\n",
      "\n",
      "   micro avg       0.02      0.47      0.04       417\n",
      "   macro avg       0.03      0.48      0.05       417\n",
      "weighted avg       0.03      0.47      0.05       417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance without clustering\n",
    "\n",
    "iou = .5\n",
    "known_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = known_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_knowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "unknown_no_cluster_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                       evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                       field_name = \"clip_RPN_predictions_no_clustering\",\n",
    "                                       fiftyone_Dataset = fo_dataset,\n",
    "                                       classes = known_unknowns,\n",
    "                                       BCC=False,\n",
    "                                       iou=iou,\n",
    "                                       labelmap=dataset_class_labels,\n",
    "                                       )\n",
    "\n",
    "print(f'Known mAP (no BCC): {known_no_cluster_evaluation.mAP()}')\n",
    "known_no_cluster_evaluation.print_report()\n",
    "print(f'Unknown mAP (no BCC): {unknown_no_cluster_evaluation.mAP()}')\n",
    "unknown_no_cluster_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.2m elapsed, 0s remaining, 9.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.5s elapsed, 0s remaining, 186.4 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.7s elapsed, 0s remaining, 186.8 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.32      0.51      0.39        63\n",
      "         vase       0.38      0.50      0.43        70\n",
      "      toaster       0.14      0.25      0.18        64\n",
      "    microwave       0.23      0.51      0.32        57\n",
      "        mouse       0.04      0.35      0.07        65\n",
      " potted plant       0.21      0.41      0.28       165\n",
      "  sports ball       0.14      0.45      0.21        94\n",
      "        zebra       0.28      0.76      0.41        88\n",
      "          dog       0.15      0.86      0.25        74\n",
      "         bird       0.18      0.88      0.31        69\n",
      "        bench       0.15      0.60      0.23        80\n",
      "parking meter       0.19      0.71      0.30        76\n",
      "     airplane       0.22      0.93      0.36        69\n",
      "      bicycle       0.32      0.78      0.45        79\n",
      "       lizard       0.41      0.56      0.47        63\n",
      "       turtle       0.31      0.52      0.39        75\n",
      "          pen       0.16      0.71      0.26       102\n",
      "   cowboy hat       0.19      0.46      0.27       108\n",
      "         tank       0.10      0.87      0.18        69\n",
      "\n",
      "    micro avg       0.16      0.60      0.25      1530\n",
      "    macro avg       0.21      0.58      0.29      1530\n",
      " weighted avg       0.21      0.60      0.30      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps = 35\n",
    "iou = .1\n",
    "\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.3m elapsed, 0s remaining, 9.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.5s elapsed, 0s remaining, 187.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.6s elapsed, 0s remaining, 187.2 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.15      0.24      0.18        63\n",
      "         vase       0.18      0.23      0.20        70\n",
      "      toaster       0.03      0.06      0.04        64\n",
      "    microwave       0.06      0.14      0.09        57\n",
      "        mouse       0.01      0.11      0.02        65\n",
      " potted plant       0.06      0.12      0.08       165\n",
      "  sports ball       0.05      0.16      0.08        94\n",
      "        zebra       0.11      0.31      0.17        88\n",
      "          dog       0.11      0.64      0.19        74\n",
      "         bird       0.11      0.52      0.18        69\n",
      "        bench       0.05      0.21      0.08        80\n",
      "parking meter       0.07      0.25      0.11        76\n",
      "     airplane       0.06      0.23      0.09        69\n",
      "      bicycle       0.12      0.30      0.18        79\n",
      "       lizard       0.09      0.13      0.11        63\n",
      "       turtle       0.16      0.27      0.20        75\n",
      "          pen       0.05      0.22      0.08       102\n",
      "   cowboy hat       0.05      0.11      0.06       108\n",
      "         tank       0.06      0.49      0.10        69\n",
      "\n",
      "    micro avg       0.06      0.24      0.10      1530\n",
      "    macro avg       0.08      0.24      0.11      1530\n",
      " weighted avg       0.08      0.24      0.11      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iou=0.5\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 394/394 [1.0m elapsed, 0s remaining, 8.0 samples/s]       \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [4.2s elapsed, 0s remaining, 240.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [3.8s elapsed, 0s remaining, 295.3 samples/s]      \n",
      "mAP = 0.00485038880779554\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.10      0.06      0.08        63\n",
      "         vase       0.10      0.07      0.08        70\n",
      "      toaster       0.00      0.00      0.00        64\n",
      "    microwave       0.04      0.05      0.05        57\n",
      "        mouse       0.02      0.08      0.03        65\n",
      " potted plant       0.04      0.06      0.05       165\n",
      "  sports ball       0.04      0.06      0.05        94\n",
      "        zebra       0.07      0.08      0.07        88\n",
      "          dog       0.05      0.11      0.07        74\n",
      "         bird       0.04      0.09      0.06        69\n",
      "        bench       0.02      0.05      0.03        80\n",
      "parking meter       0.04      0.07      0.05        76\n",
      "     airplane       0.05      0.07      0.06        69\n",
      "      bicycle       0.03      0.04      0.04        79\n",
      "       lizard       0.00      0.00      0.00        63\n",
      "       turtle       0.00      0.00      0.00        75\n",
      "          pen       0.02      0.03      0.02       102\n",
      "   cowboy hat       0.01      0.01      0.01       108\n",
      "         tank       0.04      0.13      0.06        69\n",
      "\n",
      "    micro avg       0.03      0.05      0.04      1530\n",
      "    macro avg       0.03      0.05      0.04      1530\n",
      " weighted avg       0.04      0.05      0.04      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on large objects only\n",
    "# Bboxes are in [top-left-x, top-left-y, width, height] format\n",
    "bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "small_view = item_view.filter_labels(\"ground_truth\", bbox_area < 0.2)\n",
    "# use our dataset and defined transformations\n",
    "small_object_dataset = FiftyOneTorchDataset(small_view, test_transforms,\n",
    "        classes=dataset_class_labels)\n",
    "iou=0.5\n",
    "eps = 35\n",
    "BCC_evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = small_object_dataset,\n",
    "                                field_name = f\"clip_RPN_small_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=iou,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "map = BCC_evaluation.mAP()\n",
    "print(f'mAP = {map}')\n",
    "BCC_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sensitivity Study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 5\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.3m elapsed, 0s remaining, 9.3 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.0s elapsed, 0s remaining, 202.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.1s elapsed, 0s remaining, 206.7 samples/s]      \n",
      "mAP = 0.013848216871582131\n",
      "epsilon = 10\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 9.5 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.7s elapsed, 0s remaining, 175.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.6s elapsed, 0s remaining, 191.3 samples/s]      \n",
      "mAP = 0.014490400290133697\n",
      "epsilon = 15\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 8.9 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.8s elapsed, 0s remaining, 183.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [5.8s elapsed, 0s remaining, 183.2 samples/s]      \n",
      "mAP = 0.014768878102337428\n",
      "epsilon = 20\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.4m elapsed, 0s remaining, 8.9 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.9s elapsed, 0s remaining, 168.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.1s elapsed, 0s remaining, 177.0 samples/s]      \n",
      "mAP = 0.015959547900688507\n",
      "epsilon = 25\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.5m elapsed, 0s remaining, 8.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [5.9s elapsed, 0s remaining, 175.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 174.1 samples/s]      \n",
      "mAP = 0.015211357096605739\n",
      "epsilon = 30\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.5m elapsed, 0s remaining, 8.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 164.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.4s elapsed, 0s remaining, 166.7 samples/s]      \n",
      "mAP = 0.01556719808789616\n",
      "epsilon = 35\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 165.7 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.4s elapsed, 0s remaining, 164.7 samples/s]      \n",
      "mAP = 0.016176742240240776\n",
      "epsilon = 40\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.3s elapsed, 0s remaining, 156.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.7s elapsed, 0s remaining, 162.2 samples/s]      \n",
      "mAP = 0.0158219613251373\n",
      "epsilon = 45\n",
      "Using device cuda\n",
      " 100% |███████████████| 1013/1013 [2.6m elapsed, 0s remaining, 8.3 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.5s elapsed, 0s remaining, 157.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.6s elapsed, 0s remaining, 156.5 samples/s]      \n",
      "mAP = 0.014895660763696708\n",
      "best_mAP = 0.016176742240240776\n",
      "best_eps = 35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   background       0.00      0.00      0.00         0\n",
      "        clock       0.15      0.24      0.18        63\n",
      "         vase       0.18      0.23      0.20        70\n",
      "      toaster       0.03      0.06      0.04        64\n",
      "    microwave       0.06      0.14      0.09        57\n",
      "        mouse       0.01      0.11      0.02        65\n",
      " potted plant       0.06      0.12      0.08       165\n",
      "  sports ball       0.05      0.16      0.08        94\n",
      "        zebra       0.11      0.31      0.17        88\n",
      "          dog       0.11      0.64      0.19        74\n",
      "         bird       0.11      0.52      0.18        69\n",
      "        bench       0.05      0.21      0.08        80\n",
      "parking meter       0.07      0.25      0.11        76\n",
      "     airplane       0.06      0.23      0.09        69\n",
      "      bicycle       0.12      0.30      0.18        79\n",
      "       lizard       0.09      0.13      0.11        63\n",
      "       turtle       0.16      0.27      0.20        75\n",
      "          pen       0.05      0.22      0.08       102\n",
      "   cowboy hat       0.05      0.11      0.06       108\n",
      "         tank       0.06      0.49      0.10        69\n",
      "\n",
      "    micro avg       0.06      0.24      0.10      1530\n",
      "    macro avg       0.08      0.24      0.11      1530\n",
      " weighted avg       0.08      0.24      0.11      1530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find performance with clustering\n",
    "\n",
    "results = []\n",
    "best_map = 0\n",
    "\n",
    "for eps in range(5, 50, 5):\n",
    "    print(f'epsilon = {eps}')\n",
    "\n",
    "    evaluation = evaluate_model(clip_frcnn_model,\n",
    "                                evaluation_dataset = evaluation_dataset,\n",
    "                                field_name = f\"clip_RPN_predictions_eps_{eps}\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                BCC=True,\n",
    "                                iou=0.5,\n",
    "                                eps= eps\n",
    "                                )\n",
    "\n",
    "    map = evaluation.mAP()\n",
    "    print(f'mAP = {map}')\n",
    "\n",
    "    results.append([eps, map])\n",
    "    if map > best_map:\n",
    "        best_eps = eps\n",
    "        best_map = map\n",
    "        best_eval = evaluation\n",
    "\n",
    "print(f'best_mAP = {best_map}')\n",
    "print(f'best_eps = {best_eps}')\n",
    "best_eval.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test against Faster-RCNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# create the labelmap for the coco dataset\n",
    "coco_labels = open(\"dataset_analysis/coco_labels.txt\", \"r\")\n",
    "coco_list = coco_labels.read().splitlines() # read each line in as a value in a list\n",
    "coco_list.insert(0,'background') # add the background class\n",
    "coco_id = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
    "          35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "          64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91] # annoyingly, COCO has 90 class ids but only 80 labels\n",
    "coco = dict(zip(coco_id, coco_list)) # convert it to a dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate metrics from the FRCNN using iou 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [1.9m elapsed, 0s remaining, 7.7 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [10.3s elapsed, 0s remaining, 92.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [13.8s elapsed, 0s remaining, 81.9 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [41.3s elapsed, 0s remaining, 6.6 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.3s elapsed, 0s remaining, 159.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.5s elapsed, 0s remaining, 160.6 samples/s]      \n",
      "mAP: 0.43711472188549855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.24      0.84      0.37        63\n",
      "         vase       0.17      0.90      0.28        70\n",
      "      toaster       0.52      0.53      0.53        64\n",
      "    microwave       0.31      0.79      0.44        57\n",
      "        mouse       0.36      0.80      0.50        65\n",
      " potted plant       0.16      0.81      0.26       165\n",
      "  sports ball       0.50      0.68      0.58        94\n",
      "        zebra       0.32      1.00      0.49        88\n",
      "          dog       0.38      0.99      0.55        74\n",
      "         bird       0.23      0.97      0.37        69\n",
      "        bench       0.15      0.96      0.26        80\n",
      "parking meter       0.37      0.93      0.53        76\n",
      "     airplane       0.32      1.00      0.48        69\n",
      "      bicycle       0.14      0.95      0.25        79\n",
      "\n",
      "    micro avg       0.23      0.87      0.37      1113\n",
      "    macro avg       0.30      0.87      0.42      1113\n",
      " weighted avg       0.29      0.87      0.41      1113\n",
      "\n",
      "mAP: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.00      0.00      0.00      63.0\n",
      "      turtle       0.00      0.00      0.00      75.0\n",
      "         pen       0.00      0.00      0.00     102.0\n",
      "  cowboy hat       0.00      0.00      0.00     108.0\n",
      "        tank       0.00      0.00      0.00      69.0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     417.0\n",
      "   macro avg       0.00      0.00      0.00     417.0\n",
      "weighted avg       0.00      0.00      0.00     417.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "iou=.1\n",
    "pre_trained_known_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = known_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_known_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_knowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "pre_trained_unknown_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_unknown_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_unknowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "\n",
    "print(f'mAP: {pre_trained_known_evaluation.mAP()}')\n",
    "pre_trained_known_evaluation.print_report()\n",
    "\n",
    "print(f'mAP: {pre_trained_unknown_evaluation.mAP()}')\n",
    "pre_trained_unknown_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      " 100% |█████████████████| 742/742 [2.0m elapsed, 0s remaining, 7.1 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [9.9s elapsed, 0s remaining, 100.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [12.3s elapsed, 0s remaining, 83.4 samples/s]      \n",
      "Using device cuda\n",
      " 100% |█████████████████| 271/271 [39.7s elapsed, 0s remaining, 7.4 samples/s]      \n",
      "Evaluating detections...\n",
      " 100% |███████████████| 1013/1013 [6.1s elapsed, 0s remaining, 168.0 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1013/1013 [6.2s elapsed, 0s remaining, 149.4 samples/s]      \n",
      "mAP: 0.43711472188549855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        clock       0.20      0.68      0.30        63\n",
      "         vase       0.17      0.90      0.28        70\n",
      "      toaster       0.51      0.52      0.51        64\n",
      "    microwave       0.29      0.75      0.42        57\n",
      "        mouse       0.34      0.75      0.47        65\n",
      " potted plant       0.09      0.48      0.16       165\n",
      "  sports ball       0.49      0.66      0.56        94\n",
      "        zebra       0.31      0.97      0.47        88\n",
      "          dog       0.37      0.97      0.54        74\n",
      "         bird       0.22      0.96      0.36        69\n",
      "        bench       0.14      0.91      0.25        80\n",
      "parking meter       0.31      0.79      0.44        76\n",
      "     airplane       0.31      0.96      0.46        69\n",
      "      bicycle       0.14      0.91      0.24        79\n",
      "\n",
      "    micro avg       0.21      0.78      0.33      1113\n",
      "    macro avg       0.28      0.80      0.39      1113\n",
      " weighted avg       0.26      0.78      0.37      1113\n",
      "\n",
      "mAP: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      lizard       0.00      0.00      0.00      63.0\n",
      "      turtle       0.00      0.00      0.00      75.0\n",
      "         pen       0.00      0.00      0.00     102.0\n",
      "  cowboy hat       0.00      0.00      0.00     108.0\n",
      "        tank       0.00      0.00      0.00      69.0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     417.0\n",
      "   macro avg       0.00      0.00      0.00     417.0\n",
      "weighted avg       0.00      0.00      0.00     417.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iou=.5\n",
    "pre_trained_known_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = known_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_known_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_knowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "pre_trained_unknown_evaluation = evaluate_model(model,\n",
    "                                evaluation_dataset = unknown_evaluation_dataset,\n",
    "                                field_name = \"FRCNN_unknown_predictions\",\n",
    "                                fiftyone_Dataset = fo_dataset,\n",
    "                                classes = known_unknowns,\n",
    "                                iou=iou,\n",
    "                                labelmap=coco\n",
    "                                )\n",
    "\n",
    "print(f'mAP: {pre_trained_known_evaluation.mAP()}')\n",
    "pre_trained_known_evaluation.print_report()\n",
    "\n",
    "print(f'mAP: {pre_trained_unknown_evaluation.mAP()}')\n",
    "pre_trained_unknown_evaluation.print_report()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FigureWidget({\n    'data': [{'customdata': array([0.99967682, 0.99964222, 0.99950137, 0.99922658, 0.99922658, …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bc0d326e96440ed92ef0384ae5ec496"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = pre_trained_known_evaluation.plot_pr_curves(classes=known_knowns)\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FigureWidget({\n    'data': [{'customdata': array([0.99996078, 0.59719999, 0.49619357, 0.39688331, 0.39688331, …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66067330a1244d0eb16b8295368ed06f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot2 = BCC_evaluation.plot_pr_curves(classes=known_unknowns)\n",
    "plot2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "session.view = item_view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fiftyone_pytorch_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "torch-frcnn",
   "language": "python",
   "display_name": "torch-frcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32b6ec3046e64d04b4134553dc434fe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a4788a4fd6841788b20cfbf54a3d10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5d836b94d13e459d82429606496e4d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1645bdfb02b42fba268f7000f183639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c063e7d90f6a4027b53d1b70c8c07742",
      "placeholder": "​",
      "style": "IPY_MODEL_a410071b34034a91aeda7ef1114969c2",
      "value": " 160M/160M [01:05&lt;00:00, 2.55MB/s]"
     }
    },
    "a410071b34034a91aeda7ef1114969c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acbb3df601244291b8b2fb9ea1137573": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8c6a316609d4ca5bfee139b93177ef5",
       "IPY_MODEL_a1645bdfb02b42fba268f7000f183639"
      ],
      "layout": "IPY_MODEL_32b6ec3046e64d04b4134553dc434fe0"
     }
    },
    "c063e7d90f6a4027b53d1b70c8c07742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8c6a316609d4ca5bfee139b93177ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d836b94d13e459d82429606496e4d4f",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a4788a4fd6841788b20cfbf54a3d10b",
      "value": 167502836
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}